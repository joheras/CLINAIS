{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98418b43-333d-40de-b56e-41c45bf024f7",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b3d4745-0a32-4c26-ac7b-8c9d97d780a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cbb09f7-6929-4c4d-986a-4f8335ff5e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('clinais.train.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cb85373-23dd-46b6-8089-7a1540350a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43f04bee-f5a1-4787-be29-2961ce4f0aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 781/781 [00:00<00:00, 8765.61it/s]\n"
     ]
    }
   ],
   "source": [
    "finalresult = []\n",
    "for key in tqdm(data['annotated_entries'].keys()):\n",
    "    ident = data['annotated_entries'][key]['note_id']\n",
    "    res = []\n",
    "    tags = []\n",
    "    gold = data['annotated_entries'][key]['boundary_annotation']['gold']\n",
    "    currentboundary = ''\n",
    "    for g in gold:\n",
    "        res.append(g['span'])\n",
    "        if(g['boundary'] is None):\n",
    "            tags.append('I-'+currentboundary)\n",
    "        else:\n",
    "            currentboundary = g['boundary']\n",
    "            tags.append('B-'+currentboundary)\n",
    "    finalresult.append([ident,res,tags])\n",
    "\n",
    "# finalresult    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83ec97e8-5a98-406e-92ae-e53207d7faad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "tags = [x[2] for x in finalresult]\n",
    "tags = np.unique(list(itertools.chain(*tags)))\n",
    "id2label = {}\n",
    "label2id = {}\n",
    "for i,tag in enumerate(tags):\n",
    "    id2label[i] = tag\n",
    "    label2id[tag] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8e73553-4e23-451e-aa01-cd6c1ea4d947",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalresult = [[x[0],x[1],[label2id[y] for y in x[2]]] for x in finalresult]\n",
    "#finalresult[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e3c000c-831b-4f5d-b06f-3ce31a554658",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('clinais.dev.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91f8d306-1382-432e-abf9-8df40a1d6642",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127/127 [00:00<00:00, 11258.33it/s]\n"
     ]
    }
   ],
   "source": [
    "finalresultdev = []\n",
    "for key in tqdm(data['annotated_entries'].keys()):\n",
    "    ident = data['annotated_entries'][key]['note_id']\n",
    "    res = []\n",
    "    tags = []\n",
    "    gold = data['annotated_entries'][key]['boundary_annotation']['gold']\n",
    "    currentboundary = ''\n",
    "    for g in gold:\n",
    "        res.append(g['span'])\n",
    "        if(g['boundary'] is None):\n",
    "            tags.append('I-'+currentboundary)\n",
    "        else:\n",
    "            currentboundary = g['boundary']\n",
    "            tags.append('B-'+currentboundary)\n",
    "    finalresultdev.append([ident,res,tags])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eeef76b3-23b1-4d9e-99a5-c737291e6626",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalresultdev = [[x[0],x[1],[label2id[y] for y in x[2]]] for x in finalresultdev]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be3df269-b09c-4054-8ea9-7b25ebc9751a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset,DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13ad753b-b796-4332-b5e2-e2b48f87f15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data=finalresult,columns=['id','tokens','tags'])\n",
    "dataset_train = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fab4900-9509-40e9-9f6a-d7334c1f9471",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=finalresultdev,columns=['id','tokens','tags'])\n",
    "dataset_val = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18f26fb1-34c6-4846-ba09-b9952ccfada1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict(train=dataset_train,val=dataset_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b882d9f-4bfe-480f-b4f5-060a41dcdcd6",
   "metadata": {},
   "source": [
    "# Processing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a648f87-9723-4a08-8862-138070fddf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fd9ef6f-d9fb-4752-a340-7f7bb7f14c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.021007537841796875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 616,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2ffefda0162467db57b88b809a5458f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/616 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.018402099609375,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 5069051,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68bc3f9d04e7445b8105b61e505a99de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01667642593383789,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 9096718,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4fba42e376444a7b421778aecbe6f19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "modelCheckpoint = \"xlm-roberta-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelCheckpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1500c3d7-fe6a-431f-b6f9-171424b19ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81627bc7-d56b-4e52-aaeb-8fb0566ddee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012233495712280273,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1973ba2a2da34589bfcdd94eac564edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012016534805297852,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc4e16d4c3314091b4ab1fb98173fcbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8809a5b8-e25e-478f-8399-61825f24b47d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 781\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['id', 'tokens', 'tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 127\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46773083-a305-4650-a205-f67c2c122f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d28c1426-2367-41d9-a305-55fa1e7623e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67009bec-f535-4155-9e80-aa7faa0af0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b702405b-6458-4c6e-a6a0-19d350926b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.018271446228027344,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 2244861551,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f45a6c0e834d4a30ade33d00396d8246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForTokenClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    modelCheckpoint, num_labels=len(id2label), id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bdd829ef-beac-4d3a-972f-813578a8d966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/joheras/clinico-xlm-roberta-large into local empty directory.\n",
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "/home/joheras/.local/lib/python3.10/site-packages/transformers/optimization.py:346: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 781\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4900\n",
      "  Number of trainable parameters = 558855182\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjoheras\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.11 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/joheras/CLINAIS/wandb/run-20230313_150605-2i9mqu4o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/joheras/huggingface/runs/2i9mqu4o\" target=\"_blank\">zany-flower-96</a></strong> to <a href=\"https://wandb.ai/joheras/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4900' max='4900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4900/4900 1:58:14, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.661865</td>\n",
       "      <td>0.111563</td>\n",
       "      <td>0.345538</td>\n",
       "      <td>0.168668</td>\n",
       "      <td>0.802854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.566243</td>\n",
       "      <td>0.219553</td>\n",
       "      <td>0.449657</td>\n",
       "      <td>0.295045</td>\n",
       "      <td>0.829588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.491173</td>\n",
       "      <td>0.258460</td>\n",
       "      <td>0.506865</td>\n",
       "      <td>0.342349</td>\n",
       "      <td>0.846876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.545018</td>\n",
       "      <td>0.316547</td>\n",
       "      <td>0.553776</td>\n",
       "      <td>0.402830</td>\n",
       "      <td>0.852157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.552346</td>\n",
       "      <td>0.318463</td>\n",
       "      <td>0.530892</td>\n",
       "      <td>0.398112</td>\n",
       "      <td>0.857799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.612904</td>\n",
       "      <td>0.305660</td>\n",
       "      <td>0.556064</td>\n",
       "      <td>0.394481</td>\n",
       "      <td>0.844940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.575969</td>\n",
       "      <td>0.329515</td>\n",
       "      <td>0.559497</td>\n",
       "      <td>0.414758</td>\n",
       "      <td>0.852288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.657211</td>\n",
       "      <td>0.374910</td>\n",
       "      <td>0.594966</td>\n",
       "      <td>0.459973</td>\n",
       "      <td>0.865672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.733975</td>\n",
       "      <td>0.369550</td>\n",
       "      <td>0.591533</td>\n",
       "      <td>0.454905</td>\n",
       "      <td>0.860325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.830582</td>\n",
       "      <td>0.349354</td>\n",
       "      <td>0.557208</td>\n",
       "      <td>0.429453</td>\n",
       "      <td>0.855798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.326200</td>\n",
       "      <td>0.838902</td>\n",
       "      <td>0.371386</td>\n",
       "      <td>0.573227</td>\n",
       "      <td>0.450742</td>\n",
       "      <td>0.859898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.326200</td>\n",
       "      <td>0.827782</td>\n",
       "      <td>0.387991</td>\n",
       "      <td>0.576659</td>\n",
       "      <td>0.463875</td>\n",
       "      <td>0.847892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.326200</td>\n",
       "      <td>0.805659</td>\n",
       "      <td>0.403802</td>\n",
       "      <td>0.607551</td>\n",
       "      <td>0.485153</td>\n",
       "      <td>0.866000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.326200</td>\n",
       "      <td>0.848907</td>\n",
       "      <td>0.384673</td>\n",
       "      <td>0.591533</td>\n",
       "      <td>0.466186</td>\n",
       "      <td>0.861932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.326200</td>\n",
       "      <td>0.895353</td>\n",
       "      <td>0.386785</td>\n",
       "      <td>0.596110</td>\n",
       "      <td>0.469158</td>\n",
       "      <td>0.859406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.326200</td>\n",
       "      <td>0.895073</td>\n",
       "      <td>0.392610</td>\n",
       "      <td>0.583524</td>\n",
       "      <td>0.469397</td>\n",
       "      <td>0.859406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.326200</td>\n",
       "      <td>0.971538</td>\n",
       "      <td>0.407987</td>\n",
       "      <td>0.596110</td>\n",
       "      <td>0.484426</td>\n",
       "      <td>0.862490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.326200</td>\n",
       "      <td>0.959987</td>\n",
       "      <td>0.431672</td>\n",
       "      <td>0.614416</td>\n",
       "      <td>0.507082</td>\n",
       "      <td>0.865180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.326200</td>\n",
       "      <td>0.933503</td>\n",
       "      <td>0.436948</td>\n",
       "      <td>0.622426</td>\n",
       "      <td>0.513450</td>\n",
       "      <td>0.868230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.326200</td>\n",
       "      <td>0.898793</td>\n",
       "      <td>0.417840</td>\n",
       "      <td>0.610984</td>\n",
       "      <td>0.496283</td>\n",
       "      <td>0.865606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.032300</td>\n",
       "      <td>1.044541</td>\n",
       "      <td>0.440958</td>\n",
       "      <td>0.610984</td>\n",
       "      <td>0.512230</td>\n",
       "      <td>0.863671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.032300</td>\n",
       "      <td>0.959591</td>\n",
       "      <td>0.507791</td>\n",
       "      <td>0.633867</td>\n",
       "      <td>0.563868</td>\n",
       "      <td>0.868001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.032300</td>\n",
       "      <td>1.023961</td>\n",
       "      <td>0.481034</td>\n",
       "      <td>0.638444</td>\n",
       "      <td>0.548673</td>\n",
       "      <td>0.864261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.032300</td>\n",
       "      <td>1.052796</td>\n",
       "      <td>0.536676</td>\n",
       "      <td>0.661327</td>\n",
       "      <td>0.592517</td>\n",
       "      <td>0.866656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.032300</td>\n",
       "      <td>1.078820</td>\n",
       "      <td>0.512798</td>\n",
       "      <td>0.664760</td>\n",
       "      <td>0.578974</td>\n",
       "      <td>0.871347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.032300</td>\n",
       "      <td>1.066109</td>\n",
       "      <td>0.526753</td>\n",
       "      <td>0.653318</td>\n",
       "      <td>0.583248</td>\n",
       "      <td>0.872888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.032300</td>\n",
       "      <td>1.157509</td>\n",
       "      <td>0.527574</td>\n",
       "      <td>0.656751</td>\n",
       "      <td>0.585117</td>\n",
       "      <td>0.873282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.032300</td>\n",
       "      <td>1.226704</td>\n",
       "      <td>0.492895</td>\n",
       "      <td>0.635011</td>\n",
       "      <td>0.555000</td>\n",
       "      <td>0.855306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.032300</td>\n",
       "      <td>1.093510</td>\n",
       "      <td>0.518721</td>\n",
       "      <td>0.649886</td>\n",
       "      <td>0.576943</td>\n",
       "      <td>0.871839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.032300</td>\n",
       "      <td>1.209302</td>\n",
       "      <td>0.516216</td>\n",
       "      <td>0.655606</td>\n",
       "      <td>0.577621</td>\n",
       "      <td>0.867607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>1.155604</td>\n",
       "      <td>0.522686</td>\n",
       "      <td>0.659039</td>\n",
       "      <td>0.582996</td>\n",
       "      <td>0.874955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>1.211013</td>\n",
       "      <td>0.563173</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.606860</td>\n",
       "      <td>0.868493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>1.220065</td>\n",
       "      <td>0.527273</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.574257</td>\n",
       "      <td>0.864524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>1.188355</td>\n",
       "      <td>0.516742</td>\n",
       "      <td>0.653318</td>\n",
       "      <td>0.577059</td>\n",
       "      <td>0.869182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>1.273128</td>\n",
       "      <td>0.512512</td>\n",
       "      <td>0.632723</td>\n",
       "      <td>0.566308</td>\n",
       "      <td>0.869050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>1.236580</td>\n",
       "      <td>0.505425</td>\n",
       "      <td>0.639588</td>\n",
       "      <td>0.564646</td>\n",
       "      <td>0.862195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>1.242817</td>\n",
       "      <td>0.525735</td>\n",
       "      <td>0.654462</td>\n",
       "      <td>0.583078</td>\n",
       "      <td>0.869706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>1.285315</td>\n",
       "      <td>0.529923</td>\n",
       "      <td>0.628146</td>\n",
       "      <td>0.574869</td>\n",
       "      <td>0.861243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>1.274844</td>\n",
       "      <td>0.526017</td>\n",
       "      <td>0.636156</td>\n",
       "      <td>0.575867</td>\n",
       "      <td>0.860358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>1.300620</td>\n",
       "      <td>0.538679</td>\n",
       "      <td>0.653318</td>\n",
       "      <td>0.590486</td>\n",
       "      <td>0.862457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>1.393531</td>\n",
       "      <td>0.521657</td>\n",
       "      <td>0.633867</td>\n",
       "      <td>0.572314</td>\n",
       "      <td>0.856552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>1.264426</td>\n",
       "      <td>0.515399</td>\n",
       "      <td>0.651030</td>\n",
       "      <td>0.575329</td>\n",
       "      <td>0.864556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>1.306909</td>\n",
       "      <td>0.516038</td>\n",
       "      <td>0.625858</td>\n",
       "      <td>0.565667</td>\n",
       "      <td>0.865836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>1.304680</td>\n",
       "      <td>0.516099</td>\n",
       "      <td>0.641876</td>\n",
       "      <td>0.572157</td>\n",
       "      <td>0.866459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>1.357035</td>\n",
       "      <td>0.535198</td>\n",
       "      <td>0.635011</td>\n",
       "      <td>0.580848</td>\n",
       "      <td>0.861965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>1.292357</td>\n",
       "      <td>0.523944</td>\n",
       "      <td>0.638444</td>\n",
       "      <td>0.575554</td>\n",
       "      <td>0.866196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>1.336168</td>\n",
       "      <td>0.524680</td>\n",
       "      <td>0.656751</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.862162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>1.320146</td>\n",
       "      <td>0.530120</td>\n",
       "      <td>0.654462</td>\n",
       "      <td>0.585765</td>\n",
       "      <td>0.865081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>1.341774</td>\n",
       "      <td>0.531814</td>\n",
       "      <td>0.640732</td>\n",
       "      <td>0.581214</td>\n",
       "      <td>0.867410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>1.346816</td>\n",
       "      <td>0.500454</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.557975</td>\n",
       "      <td>0.865836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>1.409379</td>\n",
       "      <td>0.540252</td>\n",
       "      <td>0.637300</td>\n",
       "      <td>0.584777</td>\n",
       "      <td>0.857274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.369721</td>\n",
       "      <td>0.530689</td>\n",
       "      <td>0.643021</td>\n",
       "      <td>0.581480</td>\n",
       "      <td>0.864819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.383966</td>\n",
       "      <td>0.551929</td>\n",
       "      <td>0.638444</td>\n",
       "      <td>0.592042</td>\n",
       "      <td>0.860915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.342142</td>\n",
       "      <td>0.541547</td>\n",
       "      <td>0.648741</td>\n",
       "      <td>0.590318</td>\n",
       "      <td>0.866032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.301107</td>\n",
       "      <td>0.541588</td>\n",
       "      <td>0.655606</td>\n",
       "      <td>0.593168</td>\n",
       "      <td>0.869575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.348748</td>\n",
       "      <td>0.549133</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.596234</td>\n",
       "      <td>0.867246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.330861</td>\n",
       "      <td>0.562749</td>\n",
       "      <td>0.646453</td>\n",
       "      <td>0.601704</td>\n",
       "      <td>0.864097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.343192</td>\n",
       "      <td>0.537572</td>\n",
       "      <td>0.638444</td>\n",
       "      <td>0.583682</td>\n",
       "      <td>0.865770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.382353</td>\n",
       "      <td>0.554664</td>\n",
       "      <td>0.632723</td>\n",
       "      <td>0.591128</td>\n",
       "      <td>0.866032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.331457</td>\n",
       "      <td>0.513463</td>\n",
       "      <td>0.632723</td>\n",
       "      <td>0.566889</td>\n",
       "      <td>0.863867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.365560</td>\n",
       "      <td>0.527169</td>\n",
       "      <td>0.632723</td>\n",
       "      <td>0.575143</td>\n",
       "      <td>0.863703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>1.346604</td>\n",
       "      <td>0.536893</td>\n",
       "      <td>0.632723</td>\n",
       "      <td>0.580882</td>\n",
       "      <td>0.862588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>1.310343</td>\n",
       "      <td>0.519816</td>\n",
       "      <td>0.645309</td>\n",
       "      <td>0.575804</td>\n",
       "      <td>0.864491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>1.430212</td>\n",
       "      <td>0.530361</td>\n",
       "      <td>0.639588</td>\n",
       "      <td>0.579876</td>\n",
       "      <td>0.855929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>1.451011</td>\n",
       "      <td>0.534972</td>\n",
       "      <td>0.647597</td>\n",
       "      <td>0.585921</td>\n",
       "      <td>0.859833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>1.347798</td>\n",
       "      <td>0.519553</td>\n",
       "      <td>0.638444</td>\n",
       "      <td>0.572895</td>\n",
       "      <td>0.865606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>1.404118</td>\n",
       "      <td>0.543585</td>\n",
       "      <td>0.635011</td>\n",
       "      <td>0.585752</td>\n",
       "      <td>0.863638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>1.365925</td>\n",
       "      <td>0.567347</td>\n",
       "      <td>0.636156</td>\n",
       "      <td>0.599784</td>\n",
       "      <td>0.870198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>1.341762</td>\n",
       "      <td>0.547317</td>\n",
       "      <td>0.641876</td>\n",
       "      <td>0.590837</td>\n",
       "      <td>0.870198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>1.363376</td>\n",
       "      <td>0.540174</td>\n",
       "      <td>0.638444</td>\n",
       "      <td>0.585212</td>\n",
       "      <td>0.865672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>1.428842</td>\n",
       "      <td>0.552297</td>\n",
       "      <td>0.646453</td>\n",
       "      <td>0.595677</td>\n",
       "      <td>0.861309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>1.395834</td>\n",
       "      <td>0.541257</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.582452</td>\n",
       "      <td>0.864261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>1.400997</td>\n",
       "      <td>0.534366</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.578920</td>\n",
       "      <td>0.868296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>1.371197</td>\n",
       "      <td>0.536122</td>\n",
       "      <td>0.645309</td>\n",
       "      <td>0.585670</td>\n",
       "      <td>0.866262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>1.343436</td>\n",
       "      <td>0.532516</td>\n",
       "      <td>0.646453</td>\n",
       "      <td>0.583979</td>\n",
       "      <td>0.870822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>1.350225</td>\n",
       "      <td>0.513993</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.566290</td>\n",
       "      <td>0.868165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>1.363858</td>\n",
       "      <td>0.533014</td>\n",
       "      <td>0.637300</td>\n",
       "      <td>0.580511</td>\n",
       "      <td>0.869149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>1.351497</td>\n",
       "      <td>0.516698</td>\n",
       "      <td>0.637300</td>\n",
       "      <td>0.570697</td>\n",
       "      <td>0.869739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>1.367662</td>\n",
       "      <td>0.522791</td>\n",
       "      <td>0.643021</td>\n",
       "      <td>0.576706</td>\n",
       "      <td>0.869083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>1.406889</td>\n",
       "      <td>0.540097</td>\n",
       "      <td>0.639588</td>\n",
       "      <td>0.585647</td>\n",
       "      <td>0.867213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>1.381260</td>\n",
       "      <td>0.530726</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.585216</td>\n",
       "      <td>0.867181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.377349</td>\n",
       "      <td>0.535478</td>\n",
       "      <td>0.647597</td>\n",
       "      <td>0.586225</td>\n",
       "      <td>0.867771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.400391</td>\n",
       "      <td>0.527885</td>\n",
       "      <td>0.628146</td>\n",
       "      <td>0.573668</td>\n",
       "      <td>0.867443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.402715</td>\n",
       "      <td>0.553171</td>\n",
       "      <td>0.648741</td>\n",
       "      <td>0.597156</td>\n",
       "      <td>0.869641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.354399</td>\n",
       "      <td>0.535130</td>\n",
       "      <td>0.636156</td>\n",
       "      <td>0.581286</td>\n",
       "      <td>0.867181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.358176</td>\n",
       "      <td>0.536680</td>\n",
       "      <td>0.636156</td>\n",
       "      <td>0.582199</td>\n",
       "      <td>0.866393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.359450</td>\n",
       "      <td>0.530029</td>\n",
       "      <td>0.636156</td>\n",
       "      <td>0.578263</td>\n",
       "      <td>0.866590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.373742</td>\n",
       "      <td>0.537055</td>\n",
       "      <td>0.638444</td>\n",
       "      <td>0.583377</td>\n",
       "      <td>0.869018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.399075</td>\n",
       "      <td>0.536822</td>\n",
       "      <td>0.633867</td>\n",
       "      <td>0.581322</td>\n",
       "      <td>0.868821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.381888</td>\n",
       "      <td>0.540019</td>\n",
       "      <td>0.640732</td>\n",
       "      <td>0.586081</td>\n",
       "      <td>0.867902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.389952</td>\n",
       "      <td>0.549461</td>\n",
       "      <td>0.641876</td>\n",
       "      <td>0.592084</td>\n",
       "      <td>0.869870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.389017</td>\n",
       "      <td>0.550881</td>\n",
       "      <td>0.644165</td>\n",
       "      <td>0.593882</td>\n",
       "      <td>0.870297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.382463</td>\n",
       "      <td>0.533649</td>\n",
       "      <td>0.644165</td>\n",
       "      <td>0.583722</td>\n",
       "      <td>0.868624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.382124</td>\n",
       "      <td>0.537002</td>\n",
       "      <td>0.647597</td>\n",
       "      <td>0.587137</td>\n",
       "      <td>0.868558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.380251</td>\n",
       "      <td>0.543708</td>\n",
       "      <td>0.647597</td>\n",
       "      <td>0.591123</td>\n",
       "      <td>0.868460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.374727</td>\n",
       "      <td>0.539486</td>\n",
       "      <td>0.648741</td>\n",
       "      <td>0.589091</td>\n",
       "      <td>0.868821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.375234</td>\n",
       "      <td>0.522263</td>\n",
       "      <td>0.644165</td>\n",
       "      <td>0.576844</td>\n",
       "      <td>0.868263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.377520</td>\n",
       "      <td>0.528037</td>\n",
       "      <td>0.646453</td>\n",
       "      <td>0.581276</td>\n",
       "      <td>0.868493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.378449</td>\n",
       "      <td>0.530019</td>\n",
       "      <td>0.646453</td>\n",
       "      <td>0.582474</td>\n",
       "      <td>0.868460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.375273</td>\n",
       "      <td>0.531515</td>\n",
       "      <td>0.646453</td>\n",
       "      <td>0.583376</td>\n",
       "      <td>0.868755</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-49\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-49/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-49/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-49/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-49/special_tokens_map.json\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/special_tokens_map.json\n",
      "Adding files tracked by Git LFS: ['tokenizer.json']. This may take a bit of time if the files are large.\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-98\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-98/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-98/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-98/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-98/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-147\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-147/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-147/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-147/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-147/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-196\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-196/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-196/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-196/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-196/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-245\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-245/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-245/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-245/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-245/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-294\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-294/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-294/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-294/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-294/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-343\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-343/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-343/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-343/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-343/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-392\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-392/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-392/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-392/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-392/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-441\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-441/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-441/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-441/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-441/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-490\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-490/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-490/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-490/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-490/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-539\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-539/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-539/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-539/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-539/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-588\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-588/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-588/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-588/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-588/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-637\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-637/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-637/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-637/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-637/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-686\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-686/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-686/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-686/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-686/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-735\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-735/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-735/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-735/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-735/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-784\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-784/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-784/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-784/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-784/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-833\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-833/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-833/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-833/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-833/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-882\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-882/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-882/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-882/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-882/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-931\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-931/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-931/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-931/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-931/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-980\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-980/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-980/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-980/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-980/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-1029\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-1029/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-1029/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-1029/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-1029/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-1078\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-1078/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-1078/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-1078/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-1078/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-1127\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-1127/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-1127/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-1127/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-1127/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-1176\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-1176/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-1176/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-1176/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-1176/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-1225\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-1225/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-1225/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-1225/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-1225/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-1274\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-1274/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-1274/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-1274/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-1274/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-1323\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-1323/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-1323/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-1323/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-1323/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-1372\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-1372/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-1372/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-1372/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-1372/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-1421\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-1421/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-1421/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-1421/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-1421/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-1470\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-1470/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-1470/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-1470/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-1470/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-1519\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-1519/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-1519/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-1519/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-1519/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-1568\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-1568/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-1568/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-1568/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-1568/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-1617\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-1617/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-1617/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-1617/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-1617/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-1666\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-1666/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-1666/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-1666/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-1666/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-1715\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-1715/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-1715/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-1715/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-1715/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-1764\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-1764/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-1764/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-1764/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-1764/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-1813\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-1813/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-1813/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-1813/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-1813/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-1862\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-1862/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-1862/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-1862/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-1862/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-1911\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-1911/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-1911/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-1911/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-1911/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-1960\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-1960/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-1960/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-1960/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-1960/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-2009\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-2009/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-2009/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-2009/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-2009/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-2058\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-2058/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-2058/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-2058/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-2058/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-2107\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-2107/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-2107/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-2107/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-2107/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-2156\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-2156/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-2156/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-2156/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-2156/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-2205\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-2205/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-2205/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-2205/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-2205/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-2254\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-2254/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-2254/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-2254/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-2254/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-2303\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-2303/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-2303/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-2303/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-2303/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-2352\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-2352/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-2352/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-2352/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-2352/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-2401\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-2401/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-2401/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-2401/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-2401/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-2450\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-2450/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-2450/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-2450/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-2450/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-2499\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-2499/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-2499/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-2499/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-2499/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-2548\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-2548/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-2548/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-2548/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-2548/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-2597\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-2597/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-2597/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-2597/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-2597/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-2646\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-2646/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-2646/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-2646/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-2646/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-2695\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-2695/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-2695/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-2695/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-2695/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-2744\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-2744/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-2744/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-2744/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-2744/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-2793\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-2793/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-2793/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-2793/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-2793/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-2842\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-2842/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-2842/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-2842/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-2842/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-2891\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-2891/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-2891/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-2891/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-2891/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-2940\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-2940/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-2940/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-2940/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-2940/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-2989\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-2989/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-2989/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-2989/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-2989/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-3038\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-3038/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-3038/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-3038/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-3038/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-3087\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-3087/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-3087/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-3087/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-3087/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-3136\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-3136/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-3136/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-3136/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-3136/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-3185\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-3185/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-3185/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-3185/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-3185/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-3234\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-3234/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-3234/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-3234/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-3234/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-3283\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-3283/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-3283/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-3283/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-3283/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-3332\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-3332/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-3332/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-3332/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-3332/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-3381\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-3381/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-3381/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-3381/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-3381/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-3430\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-3430/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-3430/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-3430/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-3430/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-3479\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-3479/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-3479/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-3479/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-3479/special_tokens_map.json\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-3528\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-3528/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-3528/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-3528/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-3528/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-3577\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-3577/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-3577/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-3577/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-3577/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-3626\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-3626/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-3626/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-3626/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-3626/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-3675\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-3675/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-3675/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-3675/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-3675/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-3724\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-3724/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-3724/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-3724/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-3724/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-3773\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-3773/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-3773/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-3773/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-3773/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-3822\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-3822/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-3822/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-3822/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-3822/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-3871\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-3871/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-3871/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-3871/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-3871/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-3920\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-3920/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-3920/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-3920/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-3920/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-3969\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-3969/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-3969/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-3969/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-3969/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-4018\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-4018/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-4018/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-4018/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-4018/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-4067\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-4067/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-4067/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-4067/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-4067/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-4116\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-4116/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-4116/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-4116/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-4116/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-4165\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-4165/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-4165/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-4165/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-4165/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-4214\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-4214/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-4214/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-4214/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-4214/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-4263\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-4263/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-4263/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-4263/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-4263/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-4312\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-4312/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-4312/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-4312/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-4312/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-4361\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-4361/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-4361/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-4361/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-4361/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-4410\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-4410/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-4410/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-4410/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-4410/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-4459\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-4459/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-4459/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-4459/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-4459/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-4508\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-4508/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-4508/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-4508/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-4508/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-4557\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-4557/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-4557/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-4557/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-4557/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-4606\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-4606/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-4606/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-4606/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-4606/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-4655\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-4655/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-4655/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-4655/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-4655/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-4704\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-4704/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-4704/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-4704/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-4704/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-4753\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-4753/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-4753/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-4753/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-4753/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-4802\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-4802/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-4802/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-4802/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-4802/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-4851\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-4851/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-4851/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-4851/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-4851/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: id, tokens, tags. If id, tokens, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large/checkpoint-4900\n",
      "Configuration saved in clinico-xlm-roberta-large/checkpoint-4900/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/checkpoint-4900/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/checkpoint-4900/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/checkpoint-4900/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from clinico-xlm-roberta-large/checkpoint-147 (score: 0.49117258191108704).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4900, training_loss=0.03788008137092907, metrics={'train_runtime': 7105.5069, 'train_samples_per_second': 10.991, 'train_steps_per_second': 0.69, 'total_flos': 7.25349671405568e+16, 'train_loss': 0.03788008137092907, 'epoch': 100.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"clinico-xlm-roberta-large\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=100,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"val\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dab37a4b-6ba8-40c2-b570-a57d653ba06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to clinico-xlm-roberta-large\n",
      "Configuration saved in clinico-xlm-roberta-large/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large/special_tokens_map.json\n",
      "Several commits (2) will be pushed upstream.\n",
      "The progress bars may be unreliable.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0178680419921875,
       "initial": 32768,
       "n": 32768,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Upload file pytorch_model.bin",
       "rate": null,
       "total": 2235560045,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3feba9191f69431baaef7be38f12b5d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 32.0k/2.08G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.017322063446044922,
       "initial": 32768,
       "n": 32768,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Upload file runs/Mar13_15-05-55_minion/events.out.tfevents.1678716363.minion.3683884.0",
       "rate": null,
       "total": 53705,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c0374eab0fd454ebd3733a7e226958d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/Mar13_15-05-55_minion/events.out.tfevents.1678716363.minion.3683884.0:  61%|######1   | 32.0k…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: Scanning LFS files of refs/heads/main for validity...        \n",
      "remote: LFS file scan complete.        \n",
      "To https://huggingface.co/joheras/clinico-xlm-roberta-large\n",
      "   5dd5a9f..3aca688  main -> main\n",
      "\n",
      "Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.5315145813734713}, {'name': 'Recall', 'type': 'recall', 'value': 0.6464530892448512}, {'name': 'F1', 'type': 'f1', 'value': 0.5833763551884358}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.8687551254715434}]}\n",
      "To https://huggingface.co/joheras/clinico-xlm-roberta-large\n",
      "   3aca688..aeb452f  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/joheras/clinico-xlm-roberta-large/commit/3aca68802d1ecd3256885346bdf723f51d839309'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e0581e1-8cb5-41b7-a7a6-97705dacd697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Mon Mar 13 17:51:24 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.76       Driver Version: 515.76       CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0 Off |                  N/A |\n",
      "| 30%   27C    P8    28W / 350W |  22088MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  On   | 00000000:61:00.0 Off |                  N/A |\n",
      "| 30%   28C    P8    24W / 350W |  14730MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  On   | 00000000:81:00.0 Off |                  N/A |\n",
      "| 30%   27C    P8    21W / 350W |      1MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce ...  On   | 00000000:E1:00.0 Off |                  N/A |\n",
      "| 30%   27C    P8    23W / 350W |      1MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   3683884      C   ...a3/envs/fastai/bin/python    22085MiB |\n",
      "|    1   N/A  N/A   3683884      C   ...a3/envs/fastai/bin/python    14727MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f6537e-a925-41f4-a14a-90db5cd282f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai]",
   "language": "python",
   "name": "conda-env-fastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
