{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98418b43-333d-40de-b56e-41c45bf024f7",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b3d4745-0a32-4c26-ac7b-8c9d97d780a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cbb09f7-6929-4c4d-986a-4f8335ff5e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('clinais.train.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cb85373-23dd-46b6-8089-7a1540350a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43f04bee-f5a1-4787-be29-2961ce4f0aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 781/781 [00:00<00:00, 424870.48it/s]\n"
     ]
    }
   ],
   "source": [
    "finalresult = []\n",
    "for key in tqdm(data['annotated_entries'].keys()):\n",
    "    ident = data['annotated_entries'][key]['note_id']\n",
    "    res = data['annotated_entries'][key]['note_text']\n",
    "    finalresult.append([ident,res])\n",
    "\n",
    "# finalresult    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e3c000c-831b-4f5d-b06f-3ce31a554658",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('clinais.dev.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91f8d306-1382-432e-abf9-8df40a1d6642",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127/127 [00:00<00:00, 486462.66it/s]\n"
     ]
    }
   ],
   "source": [
    "finalresultdev = []\n",
    "for key in tqdm(data['annotated_entries'].keys()):\n",
    "    ident = data['annotated_entries'][key]['note_id']\n",
    "    res = data['annotated_entries'][key]['note_text']\n",
    "    finalresultdev.append([ident,res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be3df269-b09c-4054-8ea9-7b25ebc9751a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset,DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13ad753b-b796-4332-b5e2-e2b48f87f15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data=finalresult,columns=['id','text'])\n",
    "dataset_train = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fab4900-9509-40e9-9f6a-d7334c1f9471",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=finalresultdev,columns=['id','text'])\n",
    "dataset_val = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18f26fb1-34c6-4846-ba09-b9952ccfada1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict(train=dataset_train,val=dataset_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b882d9f-4bfe-480f-b4f5-060a41dcdcd6",
   "metadata": {},
   "source": [
    "# Processing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a648f87-9723-4a08-8862-138070fddf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fd9ef6f-d9fb-4752-a340-7f7bb7f14c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelCheckpoint = \"xlm-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelCheckpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1500c3d7-fe6a-431f-b6f9-171424b19ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015781641006469727,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b64ebfc1c3e46cf920d6ba49fcefa83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (876 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011930704116821289,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2433b387e00841d69461855e399ed592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"text\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True,remove_columns=[\"id\",\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3d85e20-7111-41a4-b8c9-b6034206916e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "060b8c91-a51b-4a93-9245-d2af835c9ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Review 0 length: 876'\n",
      "'>>> Review 1 length: 608'\n",
      "'>>> Review 2 length: 724'\n"
     ]
    }
   ],
   "source": [
    "tokenized_samples = tokenized_dataset[\"train\"][:3]\n",
    "\n",
    "for idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\n",
    "    print(f\"'>>> Review {idx} length: {len(sample)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f26cd16-3f75-477a-b96a-fcaf455fc198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 781\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 127\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7416d9ac-34e6-45cd-9c47-5ea99a8cf901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Concatenated reviews length: 2208'\n"
     ]
    }
   ],
   "source": [
    "concatenated_examples = {\n",
    "    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()\n",
    "}\n",
    "total_length = len(concatenated_examples[\"input_ids\"])\n",
    "print(f\"'>>> Concatenated reviews length: {total_length}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0181e407-c4f8-4d22-83ce-db9c05b1a96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # Create a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfe3f02b-d454-459b-b37d-5f3af6d03b8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012041807174682617,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a44ab8f3360a4304b5a4e07826169dfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011949300765991211,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3dd1c39163143079c2e8aced8e93111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 3562\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 565\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets = tokenized_dataset.map(group_texts, batched=True)\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46773083-a305-4650-a205-f67c2c122f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6492aa3-eb74-4be2-bf87-721a52ce852d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> <s> En Mayo de 1997, una mujer de 29 años<mask> edad fue intervenida, en otro centro, de un carcinoma de la glándula supra<mask>al izquierda clínicamente<mask> funciona<mask><mask> se manifest<mask> clínicamente como molestias en el flanco izquier叹<mask> poco específicas<mask><mask> el postparto inmediato; la ecografía y la tomografía axi<mask> computerizada abdominales mostraron una masa suprarren<mask> izquierda de 10 cmts<mask>, sólida y con áreas АТО calcificación y necrosis en su interior, siendo la radiografía de tóra<mask> y la<mask>grafía ósea'\n",
      "\n",
      "'>>> normales. En los análisis, presentaba ligero nämligen de la cortisoluria (284.5 m<mask>gr.<mask>h.<mask> y de 17-OH deineesteroides en<mask>rina (12.7 mcg./24h.), sin síntomas de hipercor<mask>solismo sistémico.<mask> realizó<mask>cción completa de la tumoración, con el diagnóstico histológico de carcinoma supra<mask>al de 10 x 7 x 5 cmts. (215 gr pump.) bien<mask><mask><mask>ado<mask><mask> con invasión vascular, amplias zonas de necrosis y un índice mitótico de 5.8/50'\n"
     ]
    }
   ],
   "source": [
    "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "for sample in samples:\n",
    "    _ = sample.pop(\"word_ids\")\n",
    "\n",
    "for chunk in data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b702405b-6458-4c6e-a6a0-19d350926b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "batch_size = 8\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(lm_datasets[\"train\"]) // batch_size\n",
    "model_name = modelCheckpoint.split(\"/\")[-1]\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{model_name}-finetuned-clinais\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    push_to_hub=True,\n",
    "    fp16=True,\n",
    "    logging_steps=logging_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "646c620b-7d69-40b1-8d25-00aad25efd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(modelCheckpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf9fdec2-5360-4ee5-8f9e-6652c821e855",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/joheras/xlm-roberta-base-finetuned-clinais into local empty directory.\n",
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"val\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47a5ba24-cbcd-453b-bea0-2c0ca3d91862",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `XLMRobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 565\n",
      "  Batch size = 16\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='72' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [36/36 02:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjoheras\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/joheras/CLINAIS/wandb/run-20230315_124402-37ct5ibt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/joheras/huggingface/runs/37ct5ibt\" target=\"_blank\">feasible-vortex-108</a></strong> to <a href=\"https://wandb.ai/joheras/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 10.34\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51e3f623-605d-41d1-806d-8ca3c3565362",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `XLMRobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "/home/joheras/.local/lib/python3.10/site-packages/transformers/optimization.py:346: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3562\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1115\n",
      "  Number of trainable parameters = 278295186\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1115' max='1115' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1115/1115 10:40, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.781785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.959100</td>\n",
       "      <td>1.689564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.959100</td>\n",
       "      <td>1.619520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.705500</td>\n",
       "      <td>1.580444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.705500</td>\n",
       "      <td>1.610419</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `XLMRobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 565\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `XLMRobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 565\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to xlm-roberta-base-finetuned-clinais/checkpoint-500\n",
      "Configuration saved in xlm-roberta-base-finetuned-clinais/checkpoint-500/config.json\n",
      "Model weights saved in xlm-roberta-base-finetuned-clinais/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in xlm-roberta-base-finetuned-clinais/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in xlm-roberta-base-finetuned-clinais/checkpoint-500/special_tokens_map.json\n",
      "tokenizer config file saved in xlm-roberta-base-finetuned-clinais/tokenizer_config.json\n",
      "Special tokens file saved in xlm-roberta-base-finetuned-clinais/special_tokens_map.json\n",
      "Adding files tracked by Git LFS: ['tokenizer.json']. This may take a bit of time if the files are large.\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `XLMRobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 565\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `XLMRobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 565\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to xlm-roberta-base-finetuned-clinais/checkpoint-1000\n",
      "Configuration saved in xlm-roberta-base-finetuned-clinais/checkpoint-1000/config.json\n",
      "Model weights saved in xlm-roberta-base-finetuned-clinais/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in xlm-roberta-base-finetuned-clinais/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in xlm-roberta-base-finetuned-clinais/checkpoint-1000/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `XLMRobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 565\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1115, training_loss=1.798773439689602, metrics={'train_runtime': 640.6893, 'train_samples_per_second': 27.798, 'train_steps_per_second': 1.74, 'total_flos': 1174921493990400.0, 'train_loss': 1.798773439689602, 'epoch': 5.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a459535-f049-4e80-b6df-3eb8ea53e55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `XLMRobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 565\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [36/36 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 5.19\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dab37a4b-6ba8-40c2-b570-a57d653ba06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to xlm-roberta-base-finetuned-clinais\n",
      "Configuration saved in xlm-roberta-base-finetuned-clinais/config.json\n",
      "Model weights saved in xlm-roberta-base-finetuned-clinais/pytorch_model.bin\n",
      "tokenizer config file saved in xlm-roberta-base-finetuned-clinais/tokenizer_config.json\n",
      "Special tokens file saved in xlm-roberta-base-finetuned-clinais/special_tokens_map.json\n",
      "Several commits (2) will be pushed upstream.\n",
      "The progress bars may be unreliable.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.018799543380737305,
       "initial": 32768,
       "n": 32768,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Upload file pytorch_model.bin",
       "rate": null,
       "total": 1113254457,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36400d4a54274a89976df5418d9245e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 32.0k/1.04G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015406131744384766,
       "initial": 6196,
       "n": 6196,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Upload file runs/Mar15_12-43-39_minion/events.out.tfevents.1678880640.minion.3960056.0",
       "rate": null,
       "total": 6196,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "694c47797c84474b844e3bfb3b256318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/Mar15_12-43-39_minion/events.out.tfevents.1678880640.minion.3960056.0: 100%|##########| 6.05k…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01133275032043457,
       "initial": 311,
       "n": 311,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Upload file runs/Mar15_12-43-39_minion/events.out.tfevents.1678881295.minion.3960056.2",
       "rate": null,
       "total": 311,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6a1bd97d30449e394da9b2d1555d12f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/Mar15_12-43-39_minion/events.out.tfevents.1678881295.minion.3960056.2: 100%|##########| 311/3…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: Scanning LFS files of refs/heads/main for validity...        \n",
      "remote: LFS file scan complete.        \n",
      "To https://huggingface.co/joheras/xlm-roberta-base-finetuned-clinais\n",
      "   a058922..7a38899  main -> main\n",
      "\n",
      "Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Masked Language Modeling', 'type': 'fill-mask'}}\n",
      "To https://huggingface.co/joheras/xlm-roberta-base-finetuned-clinais\n",
      "   7a38899..64cd2e9  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/joheras/xlm-roberta-base-finetuned-clinais/commit/7a38899a0e13752ec4b23426ff6ab25c58a5973b'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bdf8a5ce-b3a0-4dd0-9ebc-cc8528a5454d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf clinico-roberta-biomedical-finetuned/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4150f82-3b64-45f1-a9c3-87a4c4ebdc23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai]",
   "language": "python",
   "name": "conda-env-fastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
