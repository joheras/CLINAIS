{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98418b43-333d-40de-b56e-41c45bf024f7",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b3d4745-0a32-4c26-ac7b-8c9d97d780a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 781/781 [00:00<00:00, 8777.89it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('clinais.train.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "finalresult = []\n",
    "for key in tqdm(data['annotated_entries'].keys()):\n",
    "    ident = data['annotated_entries'][key]['note_id']\n",
    "    res = []\n",
    "    tags = []\n",
    "    gold = data['annotated_entries'][key]['boundary_annotation']['gold']\n",
    "    currentboundary = ''\n",
    "    for g in gold:\n",
    "        res.append(g['span'])\n",
    "        if(g['boundary'] is None):\n",
    "            tags.append('I-'+currentboundary)\n",
    "        else:\n",
    "            currentboundary = g['boundary']\n",
    "            tags.append('B-'+currentboundary)\n",
    "    finalresult.append([ident,res,tags])\n",
    "\n",
    "# finalresult    \n",
    "\n",
    "import numpy as np\n",
    "import itertools\n",
    "tags = [x[2] for x in finalresult]\n",
    "tags = np.unique(list(itertools.chain(*tags)))\n",
    "id2label = {}\n",
    "label2id = {}\n",
    "for i,tag in enumerate(tags):\n",
    "    id2label[i] = tag\n",
    "    label2id[tag] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cbb09f7-6929-4c4d-986a-4f8335ff5e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joheras/.local/lib/python3.10/site-packages/datasets/dataset_dict.py:1241: FutureWarning: 'fs' was is deprecated in favor of 'storage_options' in version 2.8.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'storage_options=fs.storage_options' instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk('augmented2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b882d9f-4bfe-480f-b4f5-060a41dcdcd6",
   "metadata": {},
   "source": [
    "# Processing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a648f87-9723-4a08-8862-138070fddf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fd9ef6f-d9fb-4752-a340-7f7bb7f14c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelCheckpoint = \"joheras/bsc-bio-ehr-es-finetuned-clinais\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelCheckpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1500c3d7-fe6a-431f-b6f9-171424b19ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81627bc7-d56b-4e52-aaeb-8fb0566ddee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015415668487548828,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 4,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d07bcadb2d245318314372aa44d4823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012039661407470703,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27f92e05ccf64021b74d200475759bfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8809a5b8-e25e-478f-8399-61825f24b47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset['train'] = tokenized_dataset['train'].remove_columns('__index_level_0__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46773083-a305-4650-a205-f67c2c122f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d28c1426-2367-41d9-a305-55fa1e7623e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67009bec-f535-4155-9e80-aa7faa0af0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b702405b-6458-4c6e-a6a0-19d350926b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at joheras/bsc-bio-ehr-es-finetuned-clinais were not used when initializing RobertaForTokenClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at joheras/bsc-bio-ehr-es-finetuned-clinais and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    modelCheckpoint, num_labels=len(id2label), id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdd829ef-beac-4d3a-972f-813578a8d966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joheras/.local/lib/python3.10/site-packages/huggingface_hub/repository.py:705: FutureWarning: Creating a repository through 'clone_from' is deprecated and will be removed in v0.11.\n",
      "  warnings.warn(\n",
      "Cloning https://huggingface.co/joheras/bsc-bio-ehr-es-finetuned-clinais-augmented2 into local empty directory.\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "/home/joheras/.local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3624\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 11400\n",
      "  Number of trainable parameters = 124063502\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjoheras\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/joheras/CLINAIS/wandb/run-20230321_155822-2t5pknl4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/joheras/huggingface/runs/2t5pknl4\" target=\"_blank\">bsc-bio-ehr-es-finetuned-clinais-augmented2</a></strong> to <a href=\"https://wandb.ai/joheras/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11400' max='11400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11400/11400 1:57:03, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.755010</td>\n",
       "      <td>0.116564</td>\n",
       "      <td>0.160507</td>\n",
       "      <td>0.135051</td>\n",
       "      <td>0.793702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.657677</td>\n",
       "      <td>0.209604</td>\n",
       "      <td>0.290391</td>\n",
       "      <td>0.243471</td>\n",
       "      <td>0.812330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.592850</td>\n",
       "      <td>0.216667</td>\n",
       "      <td>0.315734</td>\n",
       "      <td>0.256983</td>\n",
       "      <td>0.819652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.583682</td>\n",
       "      <td>0.253173</td>\n",
       "      <td>0.379092</td>\n",
       "      <td>0.303594</td>\n",
       "      <td>0.837404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.504500</td>\n",
       "      <td>0.575772</td>\n",
       "      <td>0.330872</td>\n",
       "      <td>0.520591</td>\n",
       "      <td>0.404596</td>\n",
       "      <td>0.847891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.504500</td>\n",
       "      <td>0.626594</td>\n",
       "      <td>0.350603</td>\n",
       "      <td>0.521647</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>0.838450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.504500</td>\n",
       "      <td>0.632885</td>\n",
       "      <td>0.350331</td>\n",
       "      <td>0.558606</td>\n",
       "      <td>0.430606</td>\n",
       "      <td>0.848626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.504500</td>\n",
       "      <td>0.625097</td>\n",
       "      <td>0.376853</td>\n",
       "      <td>0.563886</td>\n",
       "      <td>0.451777</td>\n",
       "      <td>0.856541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.104300</td>\n",
       "      <td>0.650895</td>\n",
       "      <td>0.399858</td>\n",
       "      <td>0.594509</td>\n",
       "      <td>0.478132</td>\n",
       "      <td>0.858152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.104300</td>\n",
       "      <td>0.743136</td>\n",
       "      <td>0.375519</td>\n",
       "      <td>0.573390</td>\n",
       "      <td>0.453824</td>\n",
       "      <td>0.845319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.104300</td>\n",
       "      <td>0.700036</td>\n",
       "      <td>0.418063</td>\n",
       "      <td>0.606125</td>\n",
       "      <td>0.494828</td>\n",
       "      <td>0.859820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.104300</td>\n",
       "      <td>0.723193</td>\n",
       "      <td>0.437454</td>\n",
       "      <td>0.631468</td>\n",
       "      <td>0.516854</td>\n",
       "      <td>0.857389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.104300</td>\n",
       "      <td>0.774127</td>\n",
       "      <td>0.456809</td>\n",
       "      <td>0.619852</td>\n",
       "      <td>0.525986</td>\n",
       "      <td>0.859594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.034400</td>\n",
       "      <td>0.810799</td>\n",
       "      <td>0.466142</td>\n",
       "      <td>0.625132</td>\n",
       "      <td>0.534055</td>\n",
       "      <td>0.852782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.034400</td>\n",
       "      <td>0.792188</td>\n",
       "      <td>0.451613</td>\n",
       "      <td>0.620908</td>\n",
       "      <td>0.522899</td>\n",
       "      <td>0.861347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.034400</td>\n",
       "      <td>0.831806</td>\n",
       "      <td>0.472727</td>\n",
       "      <td>0.631468</td>\n",
       "      <td>0.540687</td>\n",
       "      <td>0.856626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.034400</td>\n",
       "      <td>0.857862</td>\n",
       "      <td>0.488673</td>\n",
       "      <td>0.637804</td>\n",
       "      <td>0.553367</td>\n",
       "      <td>0.855806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.015800</td>\n",
       "      <td>0.874521</td>\n",
       "      <td>0.462628</td>\n",
       "      <td>0.620908</td>\n",
       "      <td>0.530207</td>\n",
       "      <td>0.853941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.015800</td>\n",
       "      <td>0.895945</td>\n",
       "      <td>0.446142</td>\n",
       "      <td>0.616684</td>\n",
       "      <td>0.517730</td>\n",
       "      <td>0.853008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.015800</td>\n",
       "      <td>0.959973</td>\n",
       "      <td>0.444190</td>\n",
       "      <td>0.613516</td>\n",
       "      <td>0.515299</td>\n",
       "      <td>0.848994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.015800</td>\n",
       "      <td>0.876860</td>\n",
       "      <td>0.457757</td>\n",
       "      <td>0.629356</td>\n",
       "      <td>0.530013</td>\n",
       "      <td>0.860075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.009500</td>\n",
       "      <td>0.942662</td>\n",
       "      <td>0.450311</td>\n",
       "      <td>0.612460</td>\n",
       "      <td>0.519016</td>\n",
       "      <td>0.851566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.009500</td>\n",
       "      <td>0.918759</td>\n",
       "      <td>0.479332</td>\n",
       "      <td>0.636748</td>\n",
       "      <td>0.546939</td>\n",
       "      <td>0.858209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.009500</td>\n",
       "      <td>0.953844</td>\n",
       "      <td>0.478952</td>\n",
       "      <td>0.636748</td>\n",
       "      <td>0.546691</td>\n",
       "      <td>0.855580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.009500</td>\n",
       "      <td>0.978483</td>\n",
       "      <td>0.477456</td>\n",
       "      <td>0.626188</td>\n",
       "      <td>0.541800</td>\n",
       "      <td>0.849502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.009500</td>\n",
       "      <td>0.974484</td>\n",
       "      <td>0.478499</td>\n",
       "      <td>0.646251</td>\n",
       "      <td>0.549865</td>\n",
       "      <td>0.851425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.006600</td>\n",
       "      <td>1.016753</td>\n",
       "      <td>0.490894</td>\n",
       "      <td>0.626188</td>\n",
       "      <td>0.550348</td>\n",
       "      <td>0.854110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.006600</td>\n",
       "      <td>0.985150</td>\n",
       "      <td>0.479592</td>\n",
       "      <td>0.645195</td>\n",
       "      <td>0.550203</td>\n",
       "      <td>0.851481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.006600</td>\n",
       "      <td>1.037160</td>\n",
       "      <td>0.471094</td>\n",
       "      <td>0.636748</td>\n",
       "      <td>0.541536</td>\n",
       "      <td>0.852612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.006600</td>\n",
       "      <td>1.023357</td>\n",
       "      <td>0.501241</td>\n",
       "      <td>0.639916</td>\n",
       "      <td>0.562152</td>\n",
       "      <td>0.853036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>1.005764</td>\n",
       "      <td>0.494364</td>\n",
       "      <td>0.648363</td>\n",
       "      <td>0.560987</td>\n",
       "      <td>0.858181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>1.021228</td>\n",
       "      <td>0.477612</td>\n",
       "      <td>0.642027</td>\n",
       "      <td>0.547748</td>\n",
       "      <td>0.855863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>1.030150</td>\n",
       "      <td>0.464258</td>\n",
       "      <td>0.637804</td>\n",
       "      <td>0.537367</td>\n",
       "      <td>0.853262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>1.054451</td>\n",
       "      <td>0.473975</td>\n",
       "      <td>0.634636</td>\n",
       "      <td>0.542664</td>\n",
       "      <td>0.852866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>1.063222</td>\n",
       "      <td>0.470309</td>\n",
       "      <td>0.627244</td>\n",
       "      <td>0.537557</td>\n",
       "      <td>0.852414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>1.035142</td>\n",
       "      <td>0.492270</td>\n",
       "      <td>0.638860</td>\n",
       "      <td>0.556066</td>\n",
       "      <td>0.856767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>1.058395</td>\n",
       "      <td>0.466507</td>\n",
       "      <td>0.617740</td>\n",
       "      <td>0.531577</td>\n",
       "      <td>0.854534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>1.051340</td>\n",
       "      <td>0.469469</td>\n",
       "      <td>0.625132</td>\n",
       "      <td>0.536232</td>\n",
       "      <td>0.856626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>1.048177</td>\n",
       "      <td>0.474016</td>\n",
       "      <td>0.635692</td>\n",
       "      <td>0.543076</td>\n",
       "      <td>0.855495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>1.077689</td>\n",
       "      <td>0.489862</td>\n",
       "      <td>0.637804</td>\n",
       "      <td>0.554128</td>\n",
       "      <td>0.859029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>1.104471</td>\n",
       "      <td>0.506579</td>\n",
       "      <td>0.650475</td>\n",
       "      <td>0.569579</td>\n",
       "      <td>0.858916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>1.090966</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.644139</td>\n",
       "      <td>0.562990</td>\n",
       "      <td>0.855608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>1.124933</td>\n",
       "      <td>0.480922</td>\n",
       "      <td>0.638860</td>\n",
       "      <td>0.548753</td>\n",
       "      <td>0.854647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>1.066789</td>\n",
       "      <td>0.480614</td>\n",
       "      <td>0.628300</td>\n",
       "      <td>0.544622</td>\n",
       "      <td>0.859085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>1.069883</td>\n",
       "      <td>0.503252</td>\n",
       "      <td>0.653643</td>\n",
       "      <td>0.568672</td>\n",
       "      <td>0.858576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>1.145730</td>\n",
       "      <td>0.500412</td>\n",
       "      <td>0.640971</td>\n",
       "      <td>0.562037</td>\n",
       "      <td>0.852584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>1.122610</td>\n",
       "      <td>0.485830</td>\n",
       "      <td>0.633580</td>\n",
       "      <td>0.549954</td>\n",
       "      <td>0.850124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>1.145889</td>\n",
       "      <td>0.496314</td>\n",
       "      <td>0.639916</td>\n",
       "      <td>0.559041</td>\n",
       "      <td>0.853177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>1.151509</td>\n",
       "      <td>0.491935</td>\n",
       "      <td>0.644139</td>\n",
       "      <td>0.557842</td>\n",
       "      <td>0.850775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>1.195183</td>\n",
       "      <td>0.488746</td>\n",
       "      <td>0.642027</td>\n",
       "      <td>0.554998</td>\n",
       "      <td>0.856061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>1.138216</td>\n",
       "      <td>0.504230</td>\n",
       "      <td>0.629356</td>\n",
       "      <td>0.559887</td>\n",
       "      <td>0.860131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>1.154720</td>\n",
       "      <td>0.475736</td>\n",
       "      <td>0.631468</td>\n",
       "      <td>0.542650</td>\n",
       "      <td>0.858492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>1.208873</td>\n",
       "      <td>0.499574</td>\n",
       "      <td>0.618796</td>\n",
       "      <td>0.552830</td>\n",
       "      <td>0.851057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>1.183816</td>\n",
       "      <td>0.491028</td>\n",
       "      <td>0.635692</td>\n",
       "      <td>0.554073</td>\n",
       "      <td>0.855495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>1.207198</td>\n",
       "      <td>0.490658</td>\n",
       "      <td>0.637804</td>\n",
       "      <td>0.554637</td>\n",
       "      <td>0.857191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>1.189396</td>\n",
       "      <td>0.501221</td>\n",
       "      <td>0.650475</td>\n",
       "      <td>0.566176</td>\n",
       "      <td>0.860527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>1.219493</td>\n",
       "      <td>0.499182</td>\n",
       "      <td>0.644139</td>\n",
       "      <td>0.562471</td>\n",
       "      <td>0.854280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>1.183845</td>\n",
       "      <td>0.500816</td>\n",
       "      <td>0.648363</td>\n",
       "      <td>0.565117</td>\n",
       "      <td>0.857898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>1.210719</td>\n",
       "      <td>0.502867</td>\n",
       "      <td>0.648363</td>\n",
       "      <td>0.566421</td>\n",
       "      <td>0.856145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>1.184960</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.650475</td>\n",
       "      <td>0.565397</td>\n",
       "      <td>0.858124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>1.192646</td>\n",
       "      <td>0.500411</td>\n",
       "      <td>0.643083</td>\n",
       "      <td>0.562847</td>\n",
       "      <td>0.857107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>1.278894</td>\n",
       "      <td>0.506234</td>\n",
       "      <td>0.643083</td>\n",
       "      <td>0.566512</td>\n",
       "      <td>0.852358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>1.226834</td>\n",
       "      <td>0.493937</td>\n",
       "      <td>0.645195</td>\n",
       "      <td>0.559524</td>\n",
       "      <td>0.856230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>1.272906</td>\n",
       "      <td>0.510339</td>\n",
       "      <td>0.651531</td>\n",
       "      <td>0.572356</td>\n",
       "      <td>0.853092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>1.266746</td>\n",
       "      <td>0.512669</td>\n",
       "      <td>0.640971</td>\n",
       "      <td>0.569686</td>\n",
       "      <td>0.852979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>1.258034</td>\n",
       "      <td>0.514238</td>\n",
       "      <td>0.648363</td>\n",
       "      <td>0.573564</td>\n",
       "      <td>0.852838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>1.232337</td>\n",
       "      <td>0.483871</td>\n",
       "      <td>0.649419</td>\n",
       "      <td>0.554554</td>\n",
       "      <td>0.856711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>1.264673</td>\n",
       "      <td>0.495098</td>\n",
       "      <td>0.639916</td>\n",
       "      <td>0.558268</td>\n",
       "      <td>0.851368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>1.260445</td>\n",
       "      <td>0.517766</td>\n",
       "      <td>0.646251</td>\n",
       "      <td>0.574918</td>\n",
       "      <td>0.856343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>1.215355</td>\n",
       "      <td>0.499588</td>\n",
       "      <td>0.640971</td>\n",
       "      <td>0.561517</td>\n",
       "      <td>0.859622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>1.245155</td>\n",
       "      <td>0.506579</td>\n",
       "      <td>0.650475</td>\n",
       "      <td>0.569579</td>\n",
       "      <td>0.859170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>1.209974</td>\n",
       "      <td>0.488240</td>\n",
       "      <td>0.635692</td>\n",
       "      <td>0.552294</td>\n",
       "      <td>0.860894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>1.254923</td>\n",
       "      <td>0.493600</td>\n",
       "      <td>0.651531</td>\n",
       "      <td>0.561675</td>\n",
       "      <td>0.859000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>1.259994</td>\n",
       "      <td>0.513423</td>\n",
       "      <td>0.646251</td>\n",
       "      <td>0.572230</td>\n",
       "      <td>0.861742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>1.304349</td>\n",
       "      <td>0.493917</td>\n",
       "      <td>0.643083</td>\n",
       "      <td>0.558716</td>\n",
       "      <td>0.854591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>1.277172</td>\n",
       "      <td>0.490673</td>\n",
       "      <td>0.638860</td>\n",
       "      <td>0.555046</td>\n",
       "      <td>0.859114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>1.309133</td>\n",
       "      <td>0.495495</td>\n",
       "      <td>0.638860</td>\n",
       "      <td>0.558118</td>\n",
       "      <td>0.851792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>1.282110</td>\n",
       "      <td>0.498776</td>\n",
       "      <td>0.645195</td>\n",
       "      <td>0.562615</td>\n",
       "      <td>0.856541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>1.266783</td>\n",
       "      <td>0.498371</td>\n",
       "      <td>0.646251</td>\n",
       "      <td>0.562759</td>\n",
       "      <td>0.859085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>1.287189</td>\n",
       "      <td>0.510425</td>\n",
       "      <td>0.646251</td>\n",
       "      <td>0.570363</td>\n",
       "      <td>0.856654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>1.287741</td>\n",
       "      <td>0.491870</td>\n",
       "      <td>0.638860</td>\n",
       "      <td>0.555811</td>\n",
       "      <td>0.855269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>1.270069</td>\n",
       "      <td>0.507910</td>\n",
       "      <td>0.644139</td>\n",
       "      <td>0.567970</td>\n",
       "      <td>0.858661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>1.357852</td>\n",
       "      <td>0.497166</td>\n",
       "      <td>0.648363</td>\n",
       "      <td>0.562786</td>\n",
       "      <td>0.850011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>1.329206</td>\n",
       "      <td>0.513784</td>\n",
       "      <td>0.649419</td>\n",
       "      <td>0.573694</td>\n",
       "      <td>0.854223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>1.343692</td>\n",
       "      <td>0.496308</td>\n",
       "      <td>0.638860</td>\n",
       "      <td>0.558633</td>\n",
       "      <td>0.854647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>1.319324</td>\n",
       "      <td>0.515481</td>\n",
       "      <td>0.650475</td>\n",
       "      <td>0.575163</td>\n",
       "      <td>0.857700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>1.357852</td>\n",
       "      <td>0.491909</td>\n",
       "      <td>0.642027</td>\n",
       "      <td>0.557032</td>\n",
       "      <td>0.851962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>1.310864</td>\n",
       "      <td>0.518113</td>\n",
       "      <td>0.649419</td>\n",
       "      <td>0.576382</td>\n",
       "      <td>0.858548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>1.294302</td>\n",
       "      <td>0.487610</td>\n",
       "      <td>0.644139</td>\n",
       "      <td>0.555050</td>\n",
       "      <td>0.861460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>1.313193</td>\n",
       "      <td>0.482650</td>\n",
       "      <td>0.646251</td>\n",
       "      <td>0.552596</td>\n",
       "      <td>0.857417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>1.317626</td>\n",
       "      <td>0.505795</td>\n",
       "      <td>0.645195</td>\n",
       "      <td>0.567053</td>\n",
       "      <td>0.856315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>1.313030</td>\n",
       "      <td>0.496350</td>\n",
       "      <td>0.646251</td>\n",
       "      <td>0.561468</td>\n",
       "      <td>0.858265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.321967</td>\n",
       "      <td>0.499184</td>\n",
       "      <td>0.646251</td>\n",
       "      <td>0.563277</td>\n",
       "      <td>0.856287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.315983</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.646251</td>\n",
       "      <td>0.563795</td>\n",
       "      <td>0.860273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.315544</td>\n",
       "      <td>0.485737</td>\n",
       "      <td>0.647307</td>\n",
       "      <td>0.555002</td>\n",
       "      <td>0.861460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.322269</td>\n",
       "      <td>0.491200</td>\n",
       "      <td>0.648363</td>\n",
       "      <td>0.558944</td>\n",
       "      <td>0.857728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.313764</td>\n",
       "      <td>0.508699</td>\n",
       "      <td>0.648363</td>\n",
       "      <td>0.570102</td>\n",
       "      <td>0.859283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.315150</td>\n",
       "      <td>0.503704</td>\n",
       "      <td>0.646251</td>\n",
       "      <td>0.566142</td>\n",
       "      <td>0.859424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.313736</td>\n",
       "      <td>0.499591</td>\n",
       "      <td>0.645195</td>\n",
       "      <td>0.563134</td>\n",
       "      <td>0.859283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>1.313570</td>\n",
       "      <td>0.498776</td>\n",
       "      <td>0.645195</td>\n",
       "      <td>0.562615</td>\n",
       "      <td>0.859283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-114\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-114/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-114/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-114/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-114/special_tokens_map.json\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-228\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-228/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-228/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-228/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-228/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-342\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-342/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-342/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-342/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-342/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-456\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-456/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-456/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-456/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-456/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-570\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-570/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-570/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-570/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-570/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-684\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-684/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-684/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-684/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-684/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-798\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-798/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-798/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-798/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-798/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-912\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-912/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-912/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-912/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-912/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-1026\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-1026/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-1026/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-1026/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-1026/special_tokens_map.json\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-1140\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-1140/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-1140/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-1140/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-1140/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-1254\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-1254/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-1254/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-1254/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-1254/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-1368\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-1368/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-1368/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-1368/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-1368/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-1482\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-1482/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-1482/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-1482/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-1482/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-1596\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-1596/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-1596/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-1596/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-1596/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-1710\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-1710/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-1710/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-1710/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-1710/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-1824\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-1824/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-1824/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-1824/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-1824/special_tokens_map.json\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-1938\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-1938/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-1938/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-1938/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-1938/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-2052\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-2052/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-2052/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-2052/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-2052/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-2166\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-2166/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-2166/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-2166/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-2166/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-2280\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-2280/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-2280/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-2280/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-2280/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-2394\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-2394/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-2394/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-2394/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-2394/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-2508\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-2508/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-2508/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-2508/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-2508/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-2622\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-2622/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-2622/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-2622/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-2622/special_tokens_map.json\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-2736\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-2736/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-2736/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-2736/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-2736/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-2850\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-2850/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-2850/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-2850/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-2850/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-2964\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-2964/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-2964/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-2964/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-2964/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-3078\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-3078/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-3078/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-3078/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-3078/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-3192\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-3192/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-3192/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-3192/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-3192/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-3306\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-3306/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-3306/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-3306/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-3306/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-3420\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-3420/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-3420/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-3420/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-3420/special_tokens_map.json\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-3534\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-3534/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-3534/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-3534/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-3534/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-3648\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-3648/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-3648/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-3648/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-3648/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-3762\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-3762/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-3762/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-3762/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-3762/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-3876\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-3876/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-3876/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-3876/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-3876/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-3990\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-3990/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-3990/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-3990/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-3990/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-4104\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-4104/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-4104/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-4104/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-4104/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-4218\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-4218/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-4218/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-4218/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-4218/special_tokens_map.json\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-4332\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-4332/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-4332/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-4332/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-4332/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-4446\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-4446/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-4446/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-4446/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-4446/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-4560\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-4560/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-4560/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-4560/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-4560/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-4674\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-4674/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-4674/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-4674/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-4674/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-4788\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-4788/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-4788/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-4788/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-4788/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-4902\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-4902/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-4902/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-4902/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-4902/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-5016\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-5016/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-5016/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-5016/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-5016/special_tokens_map.json\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-5130\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-5130/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-5130/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-5130/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-5130/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-5244\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-5244/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-5244/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-5244/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-5244/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-5358\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-5358/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-5358/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-5358/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-5358/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-5472\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-5472/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-5472/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-5472/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-5472/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-5586\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-5586/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-5586/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-5586/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-5586/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-5700\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-5700/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-5700/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-5700/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-5700/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-5814\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-5814/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-5814/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-5814/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-5814/special_tokens_map.json\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-5928\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-5928/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-5928/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-5928/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-5928/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-6042\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-6042/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-6042/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-6042/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-6042/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-6156\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-6156/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-6156/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-6156/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-6156/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-6270\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-6270/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-6270/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-6270/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-6270/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-6384\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-6384/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-6384/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-6384/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-6384/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-6498\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-6498/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-6498/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-6498/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-6498/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-6612\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-6612/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-6612/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-6612/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-6612/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-6726\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-6726/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-6726/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-6726/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-6726/special_tokens_map.json\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-6840\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-6840/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-6840/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-6840/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-6840/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-6954\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-6954/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-6954/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-6954/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-6954/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-7068\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-7068/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-7068/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-7068/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-7068/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-7182\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-7182/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-7182/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-7182/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-7182/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-7296\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-7296/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-7296/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-7296/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-7296/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-7410\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-7410/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-7410/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-7410/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-7410/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-7524\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-7524/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-7524/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-7524/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-7524/special_tokens_map.json\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-7638\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-7638/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-7638/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-7638/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-7638/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-7752\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-7752/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-7752/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-7752/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-7752/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-7866\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-7866/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-7866/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-7866/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-7866/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-7980\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-7980/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-7980/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-7980/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-7980/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-8094\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-8094/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-8094/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-8094/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-8094/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-8208\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-8208/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-8208/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-8208/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-8208/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-8322\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-8322/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-8322/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-8322/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-8322/special_tokens_map.json\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-8436\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-8436/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-8436/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-8436/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-8436/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-8550\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-8550/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-8550/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-8550/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-8550/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-8664\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-8664/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-8664/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-8664/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-8664/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-8778\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-8778/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-8778/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-8778/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-8778/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-8892\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-8892/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-8892/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-8892/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-8892/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-9006\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-9006/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-9006/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-9006/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-9006/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-9120\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-9120/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-9120/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-9120/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-9120/special_tokens_map.json\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-9234\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-9234/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-9234/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-9234/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-9234/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-9348\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-9348/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-9348/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-9348/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-9348/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-9462\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-9462/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-9462/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-9462/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-9462/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-9576\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-9576/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-9576/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-9576/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-9576/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-9690\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-9690/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-9690/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-9690/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-9690/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-9804\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-9804/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-9804/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-9804/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-9804/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-9918\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-9918/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-9918/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-9918/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-9918/special_tokens_map.json\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-10032\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-10032/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-10032/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-10032/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-10032/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-10146\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-10146/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-10146/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-10146/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-10146/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-10260\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-10260/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-10260/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-10260/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-10260/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-10374\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-10374/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-10374/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-10374/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-10374/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-10488\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-10488/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-10488/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-10488/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-10488/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-10602\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-10602/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-10602/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-10602/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-10602/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-10716\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-10716/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-10716/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-10716/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-10716/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-10830\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-10830/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-10830/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-10830/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-10830/special_tokens_map.json\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-10944\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-10944/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-10944/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-10944/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-10944/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-11058\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-11058/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-11058/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-11058/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-11058/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-11172\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-11172/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-11172/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-11172/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-11172/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-11286\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-11286/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-11286/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-11286/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-11286/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: tags, tokens. If tags, tokens are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-11400\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-11400/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-11400/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-11400/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-11400/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from bsc-bio-ehr-es-finetuned-clinais-augmented2/checkpoint-570 (score: 0.5757719874382019).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=11400, training_loss=0.03091640188505775, metrics={'train_runtime': 7034.2912, 'train_samples_per_second': 51.519, 'train_steps_per_second': 1.621, 'total_flos': 9.44647381474416e+16, 'train_loss': 0.03091640188505775, 'epoch': 100.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"bsc-bio-ehr-es-finetuned-clinais-augmented2\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=100,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"val\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dab37a4b-6ba8-40c2-b570-a57d653ba06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-augmented2\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-augmented2/special_tokens_map.json\n",
      "Several commits (2) will be pushed upstream.\n",
      "The progress bars may be unreliable.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.018866300582885742,
       "initial": 32768,
       "n": 32768,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Upload file pytorch_model.bin",
       "rate": null,
       "total": 496326701,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca35692c1f7c44009f614646d21eb8e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 32.0k/473M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015863656997680664,
       "initial": 32768,
       "n": 32768,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Upload file runs/Mar21_15-58-12_minion/events.out.tfevents.1679410700.minion.474056.0",
       "rate": null,
       "total": 55715,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6546b77d93d4f62850d076cb940013c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/Mar21_15-58-12_minion/events.out.tfevents.1679410700.minion.474056.0:  59%|#####8    | 32.0k/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/joheras/bsc-bio-ehr-es-finetuned-clinais-augmented2\n",
      "   7f147fc..48c342c  main -> main\n",
      "\n",
      "Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.4987755102040816}, {'name': 'Recall', 'type': 'recall', 'value': 0.6451953537486801}, {'name': 'F1', 'type': 'f1', 'value': 0.5626151012891344}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.8592831298055179}]}\n",
      "To https://huggingface.co/joheras/bsc-bio-ehr-es-finetuned-clinais-augmented2\n",
      "   48c342c..6d5b0d7  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/joheras/bsc-bio-ehr-es-finetuned-clinais-augmented2/commit/48c342ca070b66f16b64d10ecdbb5e73ba764e59'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d58d5781-2f12-4b75-8de4-c998d6e6cbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf bsc-bio-ehr-es-finetuned-clinais-augmented2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6d4449-dfa6-45e3-a0a6-e32f2a40c9be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai]",
   "language": "python",
   "name": "conda-env-fastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
