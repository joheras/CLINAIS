{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98418b43-333d-40de-b56e-41c45bf024f7",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b3d4745-0a32-4c26-ac7b-8c9d97d780a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cbb09f7-6929-4c4d-986a-4f8335ff5e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('clinais.train.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cb85373-23dd-46b6-8089-7a1540350a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43f04bee-f5a1-4787-be29-2961ce4f0aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 781/781 [00:00<00:00, 9436.02it/s]\n"
     ]
    }
   ],
   "source": [
    "finalresult = []\n",
    "for key in tqdm(data['annotated_entries'].keys()):\n",
    "    ident = data['annotated_entries'][key]['note_id']\n",
    "    res = []\n",
    "    tags = []\n",
    "    gold = data['annotated_entries'][key]['boundary_annotation']['gold']\n",
    "    currentboundary = ''\n",
    "    for g in gold:\n",
    "        res.append(g['span'])\n",
    "        if(g['boundary'] is None):\n",
    "            tags.append('I-'+currentboundary)\n",
    "        else:\n",
    "            currentboundary = g['boundary']\n",
    "            tags.append('B-'+currentboundary)\n",
    "    finalresult.append([ident,res,tags])\n",
    "\n",
    "# finalresult    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83ec97e8-5a98-406e-92ae-e53207d7faad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "tags = [x[2] for x in finalresult]\n",
    "tags = np.unique(list(itertools.chain(*tags)))\n",
    "id2label = {}\n",
    "label2id = {}\n",
    "for i,tag in enumerate(tags):\n",
    "    id2label[i] = tag\n",
    "    label2id[tag] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8e73553-4e23-451e-aa01-cd6c1ea4d947",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalresult = [[x[0],x[1],[label2id[y] for y in x[2]]] for x in finalresult]\n",
    "#finalresult[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e3c000c-831b-4f5d-b06f-3ce31a554658",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('clinais.dev.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91f8d306-1382-432e-abf9-8df40a1d6642",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127/127 [00:00<00:00, 11479.11it/s]\n"
     ]
    }
   ],
   "source": [
    "finalresultdev = []\n",
    "for key in tqdm(data['annotated_entries'].keys()):\n",
    "    ident = data['annotated_entries'][key]['note_id']\n",
    "    res = []\n",
    "    tags = []\n",
    "    gold = data['annotated_entries'][key]['boundary_annotation']['gold']\n",
    "    currentboundary = ''\n",
    "    for g in gold:\n",
    "        res.append(g['span'])\n",
    "        if(g['boundary'] is None):\n",
    "            tags.append('I-'+currentboundary)\n",
    "        else:\n",
    "            currentboundary = g['boundary']\n",
    "            tags.append('B-'+currentboundary)\n",
    "    finalresultdev.append([ident,res,tags])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eeef76b3-23b1-4d9e-99a5-c737291e6626",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalresultdev = [[x[0],x[1],[label2id[y] for y in x[2]]] for x in finalresultdev]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be3df269-b09c-4054-8ea9-7b25ebc9751a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset,DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13ad753b-b796-4332-b5e2-e2b48f87f15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data=finalresult,columns=['id','tokens','tags'])\n",
    "dataset_train = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fab4900-9509-40e9-9f6a-d7334c1f9471",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=finalresultdev,columns=['id','tokens','tags'])\n",
    "dataset_val = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18f26fb1-34c6-4846-ba09-b9952ccfada1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict(train=dataset_train,val=dataset_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b882d9f-4bfe-480f-b4f5-060a41dcdcd6",
   "metadata": {},
   "source": [
    "# Processing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a648f87-9723-4a08-8862-138070fddf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fd9ef6f-d9fb-4752-a340-7f7bb7f14c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelCheckpoint = \"joheras/xlm-roberta-large-finetuned-clinais\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelCheckpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1500c3d7-fe6a-431f-b6f9-171424b19ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81627bc7-d56b-4e52-aaeb-8fb0566ddee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0156552791595459,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b348032d013c4f1793449e9659e35d09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.014667034149169922,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25ada700db92445e9a07dec84b44868d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8809a5b8-e25e-478f-8399-61825f24b47d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 781\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['id', 'tokens', 'tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 127\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46773083-a305-4650-a205-f67c2c122f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d28c1426-2367-41d9-a305-55fa1e7623e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67009bec-f535-4155-9e80-aa7faa0af0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b702405b-6458-4c6e-a6a0-19d350926b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016256093978881836,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 2240710265,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "710aed7869644f1f95a31671ee2ff90e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at joheras/xlm-roberta-large-finetuned-clinais were not used when initializing XLMRobertaForTokenClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at joheras/xlm-roberta-large-finetuned-clinais and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    modelCheckpoint, num_labels=len(id2label), id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bdd829ef-beac-4d3a-972f-813578a8d966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/joheras/clinico-xlm-roberta-large-finetuned into local empty directory.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.018917322158813477,
       "initial": 8192,
       "n": 8192,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Download file pytorch_model.bin",
       "rate": null,
       "total": 2235560045,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ece2691eabe342959b2da7dbbb1b6f45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file pytorch_model.bin:   0%|          | 8.00k/2.08G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0155181884765625,
       "initial": 5790,
       "n": 5790,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Download file runs/Mar15_08-48-07_minion/1678866492.937202/events.out.tfevents.1678866492.minion.3916099.1",
       "rate": null,
       "total": 5790,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7aab48d45cf4573b655c787ccfcfbc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file runs/Mar15_08-48-07_minion/1678866492.937202/events.out.tfevents.1678866492.minion.3916099.1: 10…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011303186416625977,
       "initial": 5790,
       "n": 5790,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Download file runs/Mar15_07-36-32_minion/1678862200.6369078/events.out.tfevents.1678862200.minion.3903462.1",
       "rate": null,
       "total": 5790,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "060f6a2dda04472dacf7254cfcef46ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file runs/Mar15_07-36-32_minion/1678862200.6369078/events.out.tfevents.1678862200.minion.3903462.1: 1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.014667749404907227,
       "initial": 8385,
       "n": 8385,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Download file runs/Mar15_08-48-07_minion/events.out.tfevents.1678866492.minion.3916099.0",
       "rate": null,
       "total": 19649,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e138ab4ff284fef81615dcc8c3f99e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file runs/Mar15_08-48-07_minion/events.out.tfevents.1678866492.minion.3916099.0:  43%|####2     | 8.1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011294841766357422,
       "initial": 1024,
       "n": 1024,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Clean file runs/Mar15_08-48-07_minion/1678866492.937202/events.out.tfevents.1678866492.minion.3916099.1",
       "rate": null,
       "total": 5790,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfda33ae34e747bd8eb11dfc6501be35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file runs/Mar15_08-48-07_minion/1678866492.937202/events.out.tfevents.1678866492.minion.3916099.1:  18%|…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011113166809082031,
       "initial": 1024,
       "n": 1024,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Clean file runs/Mar15_07-36-32_minion/1678862200.6369078/events.out.tfevents.1678862200.minion.3903462.1",
       "rate": null,
       "total": 5790,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caf7d38bfd8041b8af5000437954cb07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file runs/Mar15_07-36-32_minion/1678862200.6369078/events.out.tfevents.1678862200.minion.3903462.1:  18%…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011335372924804688,
       "initial": 1024,
       "n": 1024,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Clean file runs/Mar15_08-48-07_minion/events.out.tfevents.1678866492.minion.3916099.0",
       "rate": null,
       "total": 19649,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f38b1b044dbc4c9295baab428f66d8b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file runs/Mar15_08-48-07_minion/events.out.tfevents.1678866492.minion.3916099.0:   5%|5         | 1.00k/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011080503463745117,
       "initial": 5790,
       "n": 5790,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Download file runs/Mar15_07-40-39_minion/1678862444.515115/events.out.tfevents.1678862444.minion.3905698.1",
       "rate": null,
       "total": 5790,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "777c92dc970847c0957bfe6e19b5723e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file runs/Mar15_07-40-39_minion/1678862444.515115/events.out.tfevents.1678862444.minion.3905698.1: 10…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011083841323852539,
       "initial": 8385,
       "n": 8385,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Download file runs/Mar15_07-40-39_minion/events.out.tfevents.1678862444.minion.3905698.0",
       "rate": null,
       "total": 19649,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d455db5e9b496fa3b5cedf67898df4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file runs/Mar15_07-40-39_minion/events.out.tfevents.1678862444.minion.3905698.0:  43%|####2     | 8.1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011130809783935547,
       "initial": 1024,
       "n": 1024,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Clean file runs/Mar15_07-40-39_minion/1678862444.515115/events.out.tfevents.1678862444.minion.3905698.1",
       "rate": null,
       "total": 5790,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3873aaef2bf4367942c0851d2d12f0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file runs/Mar15_07-40-39_minion/1678862444.515115/events.out.tfevents.1678862444.minion.3905698.1:  18%|…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016268491744995117,
       "initial": 17815,
       "n": 17815,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Download file sentencepiece.bpe.model",
       "rate": null,
       "total": 5069051,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91071e2f2f1944a5b21cb2a0615af6e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file sentencepiece.bpe.model:   0%|          | 17.4k/4.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01106715202331543,
       "initial": 1024,
       "n": 1024,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Clean file runs/Mar15_07-40-39_minion/events.out.tfevents.1678862444.minion.3905698.0",
       "rate": null,
       "total": 19649,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b27de874ef2a426e81aa5bbec4ad2662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file runs/Mar15_07-40-39_minion/events.out.tfevents.1678862444.minion.3905698.0:   5%|5         | 1.00k/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011594772338867188,
       "initial": 12792,
       "n": 12792,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Download file tokenizer.json",
       "rate": null,
       "total": 17082758,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c7383c2e43247529f50ce01f7d70875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file tokenizer.json:   0%|          | 12.5k/16.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011138677597045898,
       "initial": 5766,
       "n": 5766,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Download file runs/Mar15_07-36-32_minion/events.out.tfevents.1678862200.minion.3903462.0",
       "rate": null,
       "total": 5766,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f48e2526e7e64b3598cb7c58e2c24b5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file runs/Mar15_07-36-32_minion/events.out.tfevents.1678862200.minion.3903462.0: 100%|##########| 5.6…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011045694351196289,
       "initial": 3579,
       "n": 3579,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Download file training_args.bin",
       "rate": null,
       "total": 3579,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d8b7d8ca4184505ba5a27f3973f449c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file training_args.bin: 100%|##########| 3.50k/3.50k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016251564025878906,
       "initial": 1024,
       "n": 1024,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Clean file runs/Mar15_07-36-32_minion/events.out.tfevents.1678862200.minion.3903462.0",
       "rate": null,
       "total": 5766,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "521fc9613f9f4e019c715150e58da438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file runs/Mar15_07-36-32_minion/events.out.tfevents.1678862200.minion.3903462.0:  18%|#7        | 1.00k/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011258363723754883,
       "initial": 1024,
       "n": 1024,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Clean file training_args.bin",
       "rate": null,
       "total": 3579,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca776fddf4c549dc8c4a5fd5d3da6d7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file training_args.bin:  29%|##8       | 1.00k/3.50k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013974905014038086,
       "initial": 1024,
       "n": 1024,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Clean file sentencepiece.bpe.model",
       "rate": null,
       "total": 5069051,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f2cf815c62e466bb5f72f1b2c417d2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file sentencepiece.bpe.model:   0%|          | 1.00k/4.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015795469284057617,
       "initial": 1024,
       "n": 1024,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Clean file tokenizer.json",
       "rate": null,
       "total": 17082758,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8bae8a64aa4296b2762287fa316624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file tokenizer.json:   0%|          | 1.00k/16.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.021406173706054688,
       "initial": 1024,
       "n": 1024,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Clean file pytorch_model.bin",
       "rate": null,
       "total": 2235560045,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32ace5ce27954cf8bc2c5366f474fd0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file pytorch_model.bin:   0%|          | 1.00k/2.08G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "/home/joheras/.local/lib/python3.10/site-packages/transformers/optimization.py:346: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 781\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4900\n",
      "  Number of trainable parameters = 558855182\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjoheras\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/joheras/CLINAIS/wandb/run-20230315_202937-4cd66g72</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/joheras/huggingface/runs/4cd66g72\" target=\"_blank\">scarlet-music-111</a></strong> to <a href=\"https://wandb.ai/joheras/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4900' max='4900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4900/4900 2:01:20, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.599280</td>\n",
       "      <td>0.224215</td>\n",
       "      <td>0.457666</td>\n",
       "      <td>0.300978</td>\n",
       "      <td>0.816762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.576157</td>\n",
       "      <td>0.236522</td>\n",
       "      <td>0.466819</td>\n",
       "      <td>0.313967</td>\n",
       "      <td>0.828014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.562665</td>\n",
       "      <td>0.246050</td>\n",
       "      <td>0.498856</td>\n",
       "      <td>0.329554</td>\n",
       "      <td>0.832573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.526019</td>\n",
       "      <td>0.302934</td>\n",
       "      <td>0.543478</td>\n",
       "      <td>0.389025</td>\n",
       "      <td>0.853403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.583800</td>\n",
       "      <td>0.305828</td>\n",
       "      <td>0.534325</td>\n",
       "      <td>0.389005</td>\n",
       "      <td>0.855864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.608459</td>\n",
       "      <td>0.336949</td>\n",
       "      <td>0.568650</td>\n",
       "      <td>0.423159</td>\n",
       "      <td>0.854486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.648066</td>\n",
       "      <td>0.353227</td>\n",
       "      <td>0.582380</td>\n",
       "      <td>0.439741</td>\n",
       "      <td>0.859144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.680910</td>\n",
       "      <td>0.352281</td>\n",
       "      <td>0.574371</td>\n",
       "      <td>0.436712</td>\n",
       "      <td>0.857963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.803953</td>\n",
       "      <td>0.386381</td>\n",
       "      <td>0.577803</td>\n",
       "      <td>0.463090</td>\n",
       "      <td>0.856848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.750515</td>\n",
       "      <td>0.377483</td>\n",
       "      <td>0.586957</td>\n",
       "      <td>0.459472</td>\n",
       "      <td>0.863539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.314300</td>\n",
       "      <td>0.802824</td>\n",
       "      <td>0.434963</td>\n",
       "      <td>0.600686</td>\n",
       "      <td>0.504565</td>\n",
       "      <td>0.860981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.314300</td>\n",
       "      <td>0.810299</td>\n",
       "      <td>0.425314</td>\n",
       "      <td>0.618993</td>\n",
       "      <td>0.504194</td>\n",
       "      <td>0.867246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.314300</td>\n",
       "      <td>0.830234</td>\n",
       "      <td>0.450567</td>\n",
       "      <td>0.636156</td>\n",
       "      <td>0.527514</td>\n",
       "      <td>0.863703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.314300</td>\n",
       "      <td>0.938478</td>\n",
       "      <td>0.458054</td>\n",
       "      <td>0.624714</td>\n",
       "      <td>0.528558</td>\n",
       "      <td>0.858291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.314300</td>\n",
       "      <td>0.940711</td>\n",
       "      <td>0.430400</td>\n",
       "      <td>0.615561</td>\n",
       "      <td>0.506591</td>\n",
       "      <td>0.863933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.314300</td>\n",
       "      <td>0.910481</td>\n",
       "      <td>0.442088</td>\n",
       "      <td>0.620137</td>\n",
       "      <td>0.516190</td>\n",
       "      <td>0.867935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.314300</td>\n",
       "      <td>0.961576</td>\n",
       "      <td>0.469072</td>\n",
       "      <td>0.624714</td>\n",
       "      <td>0.535819</td>\n",
       "      <td>0.865442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.314300</td>\n",
       "      <td>0.969489</td>\n",
       "      <td>0.479895</td>\n",
       "      <td>0.628146</td>\n",
       "      <td>0.544103</td>\n",
       "      <td>0.868033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.314300</td>\n",
       "      <td>1.019543</td>\n",
       "      <td>0.499553</td>\n",
       "      <td>0.639588</td>\n",
       "      <td>0.560963</td>\n",
       "      <td>0.873479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.314300</td>\n",
       "      <td>1.007292</td>\n",
       "      <td>0.466953</td>\n",
       "      <td>0.622426</td>\n",
       "      <td>0.533595</td>\n",
       "      <td>0.864163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.025900</td>\n",
       "      <td>1.035427</td>\n",
       "      <td>0.478336</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.544379</td>\n",
       "      <td>0.867345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.025900</td>\n",
       "      <td>1.132652</td>\n",
       "      <td>0.525773</td>\n",
       "      <td>0.641876</td>\n",
       "      <td>0.578053</td>\n",
       "      <td>0.864589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.025900</td>\n",
       "      <td>1.060452</td>\n",
       "      <td>0.505525</td>\n",
       "      <td>0.628146</td>\n",
       "      <td>0.560204</td>\n",
       "      <td>0.866754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.025900</td>\n",
       "      <td>1.011963</td>\n",
       "      <td>0.515799</td>\n",
       "      <td>0.635011</td>\n",
       "      <td>0.569231</td>\n",
       "      <td>0.865704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.025900</td>\n",
       "      <td>1.020469</td>\n",
       "      <td>0.492007</td>\n",
       "      <td>0.633867</td>\n",
       "      <td>0.554000</td>\n",
       "      <td>0.872888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.025900</td>\n",
       "      <td>1.058336</td>\n",
       "      <td>0.499543</td>\n",
       "      <td>0.625858</td>\n",
       "      <td>0.555612</td>\n",
       "      <td>0.868755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.025900</td>\n",
       "      <td>1.115659</td>\n",
       "      <td>0.506643</td>\n",
       "      <td>0.654462</td>\n",
       "      <td>0.571143</td>\n",
       "      <td>0.869805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.025900</td>\n",
       "      <td>1.104893</td>\n",
       "      <td>0.504837</td>\n",
       "      <td>0.656751</td>\n",
       "      <td>0.570860</td>\n",
       "      <td>0.869378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.025900</td>\n",
       "      <td>1.116727</td>\n",
       "      <td>0.497805</td>\n",
       "      <td>0.648741</td>\n",
       "      <td>0.563338</td>\n",
       "      <td>0.868526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.025900</td>\n",
       "      <td>1.161388</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.662471</td>\n",
       "      <td>0.569882</td>\n",
       "      <td>0.864425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>1.152116</td>\n",
       "      <td>0.499115</td>\n",
       "      <td>0.645309</td>\n",
       "      <td>0.562874</td>\n",
       "      <td>0.864720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>1.195075</td>\n",
       "      <td>0.493838</td>\n",
       "      <td>0.641876</td>\n",
       "      <td>0.558209</td>\n",
       "      <td>0.866065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>1.204362</td>\n",
       "      <td>0.481545</td>\n",
       "      <td>0.641876</td>\n",
       "      <td>0.550270</td>\n",
       "      <td>0.867607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>1.195199</td>\n",
       "      <td>0.524245</td>\n",
       "      <td>0.655606</td>\n",
       "      <td>0.582613</td>\n",
       "      <td>0.871215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>1.159829</td>\n",
       "      <td>0.528285</td>\n",
       "      <td>0.662471</td>\n",
       "      <td>0.587817</td>\n",
       "      <td>0.876759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>1.171592</td>\n",
       "      <td>0.522132</td>\n",
       "      <td>0.661327</td>\n",
       "      <td>0.583544</td>\n",
       "      <td>0.872035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>1.212685</td>\n",
       "      <td>0.523633</td>\n",
       "      <td>0.646453</td>\n",
       "      <td>0.578597</td>\n",
       "      <td>0.870691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>1.274703</td>\n",
       "      <td>0.525926</td>\n",
       "      <td>0.649886</td>\n",
       "      <td>0.581372</td>\n",
       "      <td>0.869182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>1.239692</td>\n",
       "      <td>0.536313</td>\n",
       "      <td>0.659039</td>\n",
       "      <td>0.591376</td>\n",
       "      <td>0.867607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>1.235776</td>\n",
       "      <td>0.547710</td>\n",
       "      <td>0.656751</td>\n",
       "      <td>0.597294</td>\n",
       "      <td>0.874627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>1.233154</td>\n",
       "      <td>0.536744</td>\n",
       "      <td>0.660183</td>\n",
       "      <td>0.592099</td>\n",
       "      <td>0.874463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>1.223868</td>\n",
       "      <td>0.510619</td>\n",
       "      <td>0.660183</td>\n",
       "      <td>0.575848</td>\n",
       "      <td>0.868493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>1.216277</td>\n",
       "      <td>0.522415</td>\n",
       "      <td>0.653318</td>\n",
       "      <td>0.580580</td>\n",
       "      <td>0.867935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>1.233476</td>\n",
       "      <td>0.534949</td>\n",
       "      <td>0.656751</td>\n",
       "      <td>0.589625</td>\n",
       "      <td>0.869411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>1.337377</td>\n",
       "      <td>0.534838</td>\n",
       "      <td>0.623570</td>\n",
       "      <td>0.575806</td>\n",
       "      <td>0.868033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>1.228686</td>\n",
       "      <td>0.541746</td>\n",
       "      <td>0.653318</td>\n",
       "      <td>0.592324</td>\n",
       "      <td>0.872987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>1.226848</td>\n",
       "      <td>0.553588</td>\n",
       "      <td>0.679634</td>\n",
       "      <td>0.610169</td>\n",
       "      <td>0.878858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>1.215342</td>\n",
       "      <td>0.497400</td>\n",
       "      <td>0.656751</td>\n",
       "      <td>0.566075</td>\n",
       "      <td>0.873676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>1.217988</td>\n",
       "      <td>0.522212</td>\n",
       "      <td>0.659039</td>\n",
       "      <td>0.582701</td>\n",
       "      <td>0.874725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>1.290553</td>\n",
       "      <td>0.550049</td>\n",
       "      <td>0.647597</td>\n",
       "      <td>0.594850</td>\n",
       "      <td>0.869772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>1.254678</td>\n",
       "      <td>0.538610</td>\n",
       "      <td>0.638444</td>\n",
       "      <td>0.584293</td>\n",
       "      <td>0.868558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>1.279210</td>\n",
       "      <td>0.530689</td>\n",
       "      <td>0.643021</td>\n",
       "      <td>0.581480</td>\n",
       "      <td>0.868066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>1.197194</td>\n",
       "      <td>0.503986</td>\n",
       "      <td>0.651030</td>\n",
       "      <td>0.568148</td>\n",
       "      <td>0.870494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>1.218929</td>\n",
       "      <td>0.521461</td>\n",
       "      <td>0.653318</td>\n",
       "      <td>0.579990</td>\n",
       "      <td>0.878202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>1.223927</td>\n",
       "      <td>0.560194</td>\n",
       "      <td>0.660183</td>\n",
       "      <td>0.606092</td>\n",
       "      <td>0.878858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>1.262032</td>\n",
       "      <td>0.540968</td>\n",
       "      <td>0.664760</td>\n",
       "      <td>0.596509</td>\n",
       "      <td>0.877251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>1.282849</td>\n",
       "      <td>0.551257</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.597484</td>\n",
       "      <td>0.874660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>1.263303</td>\n",
       "      <td>0.551791</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.597798</td>\n",
       "      <td>0.874889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>1.261926</td>\n",
       "      <td>0.535618</td>\n",
       "      <td>0.679634</td>\n",
       "      <td>0.599092</td>\n",
       "      <td>0.873774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>1.207589</td>\n",
       "      <td>0.538532</td>\n",
       "      <td>0.671625</td>\n",
       "      <td>0.597760</td>\n",
       "      <td>0.877514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>1.299550</td>\n",
       "      <td>0.535680</td>\n",
       "      <td>0.644165</td>\n",
       "      <td>0.584935</td>\n",
       "      <td>0.868591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.261439</td>\n",
       "      <td>0.548266</td>\n",
       "      <td>0.669336</td>\n",
       "      <td>0.602782</td>\n",
       "      <td>0.877284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.271284</td>\n",
       "      <td>0.552354</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.600522</td>\n",
       "      <td>0.875742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.291983</td>\n",
       "      <td>0.555019</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.602094</td>\n",
       "      <td>0.873938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.331875</td>\n",
       "      <td>0.562261</td>\n",
       "      <td>0.671625</td>\n",
       "      <td>0.612096</td>\n",
       "      <td>0.871314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.334503</td>\n",
       "      <td>0.543292</td>\n",
       "      <td>0.653318</td>\n",
       "      <td>0.593247</td>\n",
       "      <td>0.872035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.314583</td>\n",
       "      <td>0.530516</td>\n",
       "      <td>0.646453</td>\n",
       "      <td>0.582775</td>\n",
       "      <td>0.865672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.335370</td>\n",
       "      <td>0.545195</td>\n",
       "      <td>0.655606</td>\n",
       "      <td>0.595325</td>\n",
       "      <td>0.869116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.347376</td>\n",
       "      <td>0.551887</td>\n",
       "      <td>0.669336</td>\n",
       "      <td>0.604964</td>\n",
       "      <td>0.875906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.349766</td>\n",
       "      <td>0.540338</td>\n",
       "      <td>0.659039</td>\n",
       "      <td>0.593814</td>\n",
       "      <td>0.868591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>1.333981</td>\n",
       "      <td>0.538749</td>\n",
       "      <td>0.660183</td>\n",
       "      <td>0.593316</td>\n",
       "      <td>0.874922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>1.347487</td>\n",
       "      <td>0.561471</td>\n",
       "      <td>0.663616</td>\n",
       "      <td>0.608285</td>\n",
       "      <td>0.874463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>1.352952</td>\n",
       "      <td>0.542484</td>\n",
       "      <td>0.664760</td>\n",
       "      <td>0.597429</td>\n",
       "      <td>0.874627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>1.349361</td>\n",
       "      <td>0.549149</td>\n",
       "      <td>0.664760</td>\n",
       "      <td>0.601449</td>\n",
       "      <td>0.873774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>1.336839</td>\n",
       "      <td>0.561951</td>\n",
       "      <td>0.659039</td>\n",
       "      <td>0.606635</td>\n",
       "      <td>0.874856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>1.338206</td>\n",
       "      <td>0.546742</td>\n",
       "      <td>0.662471</td>\n",
       "      <td>0.599069</td>\n",
       "      <td>0.875217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>1.348576</td>\n",
       "      <td>0.537665</td>\n",
       "      <td>0.653318</td>\n",
       "      <td>0.589876</td>\n",
       "      <td>0.875873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>1.348537</td>\n",
       "      <td>0.548263</td>\n",
       "      <td>0.649886</td>\n",
       "      <td>0.594764</td>\n",
       "      <td>0.873052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>1.351201</td>\n",
       "      <td>0.534017</td>\n",
       "      <td>0.655606</td>\n",
       "      <td>0.588598</td>\n",
       "      <td>0.875119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>1.348571</td>\n",
       "      <td>0.551331</td>\n",
       "      <td>0.663616</td>\n",
       "      <td>0.602285</td>\n",
       "      <td>0.877153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>1.352955</td>\n",
       "      <td>0.548141</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.598024</td>\n",
       "      <td>0.877186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.394010</td>\n",
       "      <td>0.553606</td>\n",
       "      <td>0.649886</td>\n",
       "      <td>0.597895</td>\n",
       "      <td>0.875119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.365704</td>\n",
       "      <td>0.529577</td>\n",
       "      <td>0.645309</td>\n",
       "      <td>0.581743</td>\n",
       "      <td>0.874233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.353765</td>\n",
       "      <td>0.541222</td>\n",
       "      <td>0.638444</td>\n",
       "      <td>0.585827</td>\n",
       "      <td>0.871904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.355021</td>\n",
       "      <td>0.541829</td>\n",
       "      <td>0.637300</td>\n",
       "      <td>0.585699</td>\n",
       "      <td>0.869280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.380961</td>\n",
       "      <td>0.518657</td>\n",
       "      <td>0.636156</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.868493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.362519</td>\n",
       "      <td>0.536984</td>\n",
       "      <td>0.639588</td>\n",
       "      <td>0.583812</td>\n",
       "      <td>0.870658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.360506</td>\n",
       "      <td>0.538905</td>\n",
       "      <td>0.641876</td>\n",
       "      <td>0.585901</td>\n",
       "      <td>0.871248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.361648</td>\n",
       "      <td>0.538830</td>\n",
       "      <td>0.643021</td>\n",
       "      <td>0.586333</td>\n",
       "      <td>0.871084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.356030</td>\n",
       "      <td>0.543078</td>\n",
       "      <td>0.641876</td>\n",
       "      <td>0.588359</td>\n",
       "      <td>0.871871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.355825</td>\n",
       "      <td>0.539866</td>\n",
       "      <td>0.643021</td>\n",
       "      <td>0.586945</td>\n",
       "      <td>0.871642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.358576</td>\n",
       "      <td>0.534221</td>\n",
       "      <td>0.643021</td>\n",
       "      <td>0.583593</td>\n",
       "      <td>0.871707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.372743</td>\n",
       "      <td>0.534906</td>\n",
       "      <td>0.648741</td>\n",
       "      <td>0.586350</td>\n",
       "      <td>0.871609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.380997</td>\n",
       "      <td>0.553922</td>\n",
       "      <td>0.646453</td>\n",
       "      <td>0.596621</td>\n",
       "      <td>0.870658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.381287</td>\n",
       "      <td>0.554028</td>\n",
       "      <td>0.645309</td>\n",
       "      <td>0.596195</td>\n",
       "      <td>0.870461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.387905</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.648741</td>\n",
       "      <td>0.602550</td>\n",
       "      <td>0.869936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.388587</td>\n",
       "      <td>0.561386</td>\n",
       "      <td>0.648741</td>\n",
       "      <td>0.601911</td>\n",
       "      <td>0.870166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.381053</td>\n",
       "      <td>0.547042</td>\n",
       "      <td>0.645309</td>\n",
       "      <td>0.592126</td>\n",
       "      <td>0.870428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.378765</td>\n",
       "      <td>0.540748</td>\n",
       "      <td>0.645309</td>\n",
       "      <td>0.588419</td>\n",
       "      <td>0.870855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.378780</td>\n",
       "      <td>0.541787</td>\n",
       "      <td>0.645309</td>\n",
       "      <td>0.589034</td>\n",
       "      <td>0.870822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-49\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-49/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-49/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-49/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-49/special_tokens_map.json\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-98\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-98/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-98/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-98/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-98/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-147\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-147/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-147/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-147/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-147/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-196\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-196/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-196/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-196/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-196/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-245\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-245/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-245/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-245/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-245/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-294\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-294/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-294/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-294/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-294/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-343\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-343/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-343/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-343/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-343/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-392\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-392/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-392/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-392/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-392/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-441\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-441/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-441/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-441/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-441/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-490\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-490/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-490/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-490/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-490/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-539\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-539/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-539/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-539/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-539/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-588\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-588/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-588/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-588/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-588/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-637\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-637/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-637/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-637/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-637/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-686\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-686/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-686/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-686/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-686/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-735\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-735/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-735/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-735/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-735/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-784\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-784/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-784/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-784/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-784/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-833\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-833/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-833/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-833/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-833/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-882\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-882/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-882/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-882/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-882/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-931\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-931/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-931/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-931/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-931/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-980\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-980/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-980/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-980/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-980/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-1029\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-1029/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-1029/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-1029/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-1029/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-1078\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-1078/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-1078/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-1078/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-1078/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-1127\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-1127/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-1127/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-1127/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-1127/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-1176\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-1176/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-1176/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-1176/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-1176/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-1225\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-1225/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-1225/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-1225/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-1225/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-1274\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-1274/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-1274/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-1274/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-1274/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-1323\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-1323/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-1323/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-1323/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-1323/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-1372\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-1372/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-1372/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-1372/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-1372/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-1421\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-1421/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-1421/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-1421/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-1421/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-1470\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-1470/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-1470/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-1470/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-1470/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-1519\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-1519/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-1519/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-1519/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-1519/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-1568\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-1568/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-1568/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-1568/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-1568/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-1617\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-1617/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-1617/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-1617/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-1617/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-1666\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-1666/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-1666/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-1666/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-1666/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-1715\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-1715/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-1715/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-1715/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-1715/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-1764\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-1764/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-1764/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-1764/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-1764/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-1813\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-1813/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-1813/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-1813/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-1813/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-1862\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-1862/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-1862/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-1862/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-1862/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-1911\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-1911/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-1911/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-1911/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-1911/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-1960\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-1960/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-1960/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-1960/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-1960/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-2009\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-2009/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-2009/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-2009/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-2009/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-2058\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-2058/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-2058/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-2058/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-2058/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-2107\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-2107/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-2107/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-2107/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-2107/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-2156\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-2156/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-2156/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-2156/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-2156/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-2205\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-2205/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-2205/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-2205/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-2205/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-2254\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-2254/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-2254/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-2254/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-2254/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-2303\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-2303/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-2303/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-2303/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-2303/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-2352\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-2352/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-2352/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-2352/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-2352/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-2401\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-2401/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-2401/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-2401/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-2401/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-2450\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-2450/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-2450/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-2450/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-2450/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-2499\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-2499/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-2499/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-2499/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-2499/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-2548\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-2548/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-2548/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-2548/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-2548/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-2597\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-2597/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-2597/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-2597/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-2597/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-2646\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-2646/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-2646/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-2646/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-2646/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-2695\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-2695/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-2695/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-2695/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-2695/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-2744\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-2744/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-2744/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-2744/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-2744/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-2793\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-2793/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-2793/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-2793/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-2793/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-2842\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-2842/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-2842/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-2842/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-2842/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-2891\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-2891/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-2891/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-2891/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-2891/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-2940\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-2940/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-2940/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-2940/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-2940/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-2989\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-2989/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-2989/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-2989/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-2989/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-3038\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-3038/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-3038/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-3038/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-3038/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-3087\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-3087/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-3087/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-3087/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-3087/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-3136\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-3136/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-3136/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-3136/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-3136/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-3185\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-3185/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-3185/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-3185/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-3185/special_tokens_map.json\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-3234\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-3234/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-3234/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-3234/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-3234/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-3283\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-3283/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-3283/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-3283/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-3283/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-3332\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-3332/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-3332/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-3332/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-3332/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-3381\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-3381/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-3381/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-3381/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-3381/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-3430\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-3430/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-3430/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-3430/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-3430/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-3479\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-3479/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-3479/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-3479/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-3479/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-3528\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-3528/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-3528/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-3528/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-3528/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-3577\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-3577/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-3577/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-3577/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-3577/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-3626\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-3626/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-3626/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-3626/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-3626/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-3675\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-3675/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-3675/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-3675/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-3675/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-3724\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-3724/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-3724/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-3724/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-3724/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-3773\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-3773/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-3773/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-3773/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-3773/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-3822\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-3822/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-3822/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-3822/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-3822/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-3871\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-3871/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-3871/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-3871/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-3871/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-3920\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-3920/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-3920/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-3920/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-3920/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-3969\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-3969/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-3969/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-3969/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-3969/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-4018\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-4018/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-4018/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-4018/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-4018/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-4067\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-4067/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-4067/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-4067/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-4067/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-4116\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-4116/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-4116/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-4116/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-4116/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-4165\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-4165/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-4165/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-4165/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-4165/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-4214\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-4214/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-4214/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-4214/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-4214/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-4263\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-4263/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-4263/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-4263/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-4263/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-4312\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-4312/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-4312/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-4312/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-4312/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-4361\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-4361/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-4361/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-4361/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-4361/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-4410\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-4410/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-4410/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-4410/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-4410/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-4459\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-4459/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-4459/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-4459/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-4459/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-4508\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-4508/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-4508/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-4508/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-4508/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-4557\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-4557/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-4557/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-4557/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-4557/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-4606\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-4606/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-4606/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-4606/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-4606/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-4655\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-4655/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-4655/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-4655/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-4655/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-4704\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-4704/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-4704/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-4704/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-4704/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-4753\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-4753/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-4753/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-4753/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-4753/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-4802\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-4802/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-4802/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-4802/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-4802/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-4851\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-4851/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-4851/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-4851/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-4851/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, id, tags. If tokens, id, tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 127\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned/checkpoint-4900\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/checkpoint-4900/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/checkpoint-4900/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/checkpoint-4900/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/checkpoint-4900/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from clinico-xlm-roberta-large-finetuned/checkpoint-196 (score: 0.5260186791419983).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4900, training_loss=0.035870392730041424, metrics={'train_runtime': 7290.9678, 'train_samples_per_second': 10.712, 'train_steps_per_second': 0.672, 'total_flos': 7.25349671405568e+16, 'train_loss': 0.035870392730041424, 'epoch': 100.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"clinico-xlm-roberta-large-finetuned\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=100,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"val\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dab37a4b-6ba8-40c2-b570-a57d653ba06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to clinico-xlm-roberta-large-finetuned\n",
      "Configuration saved in clinico-xlm-roberta-large-finetuned/config.json\n",
      "Model weights saved in clinico-xlm-roberta-large-finetuned/pytorch_model.bin\n",
      "tokenizer config file saved in clinico-xlm-roberta-large-finetuned/tokenizer_config.json\n",
      "Special tokens file saved in clinico-xlm-roberta-large-finetuned/special_tokens_map.json\n",
      "Several commits (2) will be pushed upstream.\n",
      "The progress bars may be unreliable.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.017776966094970703,
       "initial": 32768,
       "n": 32768,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Upload file pytorch_model.bin",
       "rate": null,
       "total": 2235560045,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c6656412fc94111b9563fe3b8fa82a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 32.0k/2.08G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.017678260803222656,
       "initial": 32768,
       "n": 32768,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Upload file runs/Mar15_20-23-36_minion/events.out.tfevents.1678908575.minion.4017099.0",
       "rate": null,
       "total": 53789,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a33728750e4cf09069bd2d8b07e3dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/Mar15_20-23-36_minion/events.out.tfevents.1678908575.minion.4017099.0:  61%|######    | 32.0k…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: Scanning LFS files of refs/heads/main for validity...        \n",
      "remote: LFS file scan complete.        \n",
      "To https://huggingface.co/joheras/clinico-xlm-roberta-large-finetuned\n",
      "   d79c48e..63438aa  main -> main\n",
      "\n",
      "Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Token Classification', 'type': 'token-classification'}, 'metrics': [{'name': 'Precision', 'type': 'precision', 'value': 0.5417867435158501}, {'name': 'Recall', 'type': 'recall', 'value': 0.6453089244851259}, {'name': 'F1', 'type': 'f1', 'value': 0.5890339425587467}, {'name': 'Accuracy', 'type': 'accuracy', 'value': 0.870821715597835}]}\n",
      "To https://huggingface.co/joheras/clinico-xlm-roberta-large-finetuned\n",
      "   63438aa..a0d03c0  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/joheras/clinico-xlm-roberta-large-finetuned/commit/63438aa372b4c090265b75c0093dade6c7604136'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2f6537e-a925-41f4-a14a-90db5cd282f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf xlm-roberta-large-finetuned-clinais/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85855491-a37a-4471-8531-a43c12e39877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai]",
   "language": "python",
   "name": "conda-env-fastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
