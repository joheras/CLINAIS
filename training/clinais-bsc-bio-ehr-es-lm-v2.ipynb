{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98418b43-333d-40de-b56e-41c45bf024f7",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b3d4745-0a32-4c26-ac7b-8c9d97d780a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cbb09f7-6929-4c4d-986a-4f8335ff5e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('clinais.train.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cb85373-23dd-46b6-8089-7a1540350a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43f04bee-f5a1-4787-be29-2961ce4f0aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 781/781 [00:00<00:00, 503342.26it/s]\n"
     ]
    }
   ],
   "source": [
    "finalresult = []\n",
    "for key in tqdm(data['annotated_entries'].keys()):\n",
    "    ident = data['annotated_entries'][key]['note_id']\n",
    "    res = data['annotated_entries'][key]['note_text']\n",
    "    finalresult.append([ident,res])\n",
    "\n",
    "# finalresult    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "923e6bfe-3dd1-4506-8075-434e1d630f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('clinais.test&background.blind.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dac981d-befb-4c38-ba1c-dacf17555fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2843/2843 [00:00<00:00, 44739.96it/s]\n"
     ]
    }
   ],
   "source": [
    "for key in tqdm(data['annotated_entries'].keys()):\n",
    "    ident = data['annotated_entries'][key]['note_id']\n",
    "    res = data['annotated_entries'][key]['note_text']\n",
    "    finalresult.append([ident,res])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e3c000c-831b-4f5d-b06f-3ce31a554658",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('clinais.dev.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91f8d306-1382-432e-abf9-8df40a1d6642",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 127/127 [00:00<00:00, 393409.61it/s]\n"
     ]
    }
   ],
   "source": [
    "finalresultdev = []\n",
    "for key in tqdm(data['annotated_entries'].keys()):\n",
    "    ident = data['annotated_entries'][key]['note_id']\n",
    "    res = data['annotated_entries'][key]['note_text']\n",
    "    finalresultdev.append([ident,res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be3df269-b09c-4054-8ea9-7b25ebc9751a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset,DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13ad753b-b796-4332-b5e2-e2b48f87f15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data=finalresult,columns=['id','text'])\n",
    "dataset_train = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fab4900-9509-40e9-9f6a-d7334c1f9471",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=finalresultdev,columns=['id','text'])\n",
    "dataset_val = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18f26fb1-34c6-4846-ba09-b9952ccfada1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict(train=dataset_train,val=dataset_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b882d9f-4bfe-480f-b4f5-060a41dcdcd6",
   "metadata": {},
   "source": [
    "# Processing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a648f87-9723-4a08-8862-138070fddf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fd9ef6f-d9fb-4752-a340-7f7bb7f14c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelCheckpoint = \"PlanTL-GOB-ES/bsc-bio-ehr-es\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelCheckpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1500c3d7-fe6a-431f-b6f9-171424b19ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015877962112426758,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 4,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c5fddb36edc41d1a11d1e4e82e9d6f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (659 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01202535629272461,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "734787647a074f67b020ff2226329f1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"text\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True,remove_columns=[\"id\",\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3d85e20-7111-41a4-b8c9-b6034206916e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "060b8c91-a51b-4a93-9245-d2af835c9ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Review 0 length: 659'\n",
      "'>>> Review 1 length: 458'\n",
      "'>>> Review 2 length: 544'\n"
     ]
    }
   ],
   "source": [
    "tokenized_samples = tokenized_dataset[\"train\"][:3]\n",
    "\n",
    "for idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\n",
    "    print(f\"'>>> Review {idx} length: {len(sample)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f26cd16-3f75-477a-b96a-fcaf455fc198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 3624\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 127\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7416d9ac-34e6-45cd-9c47-5ea99a8cf901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> Concatenated reviews length: 1661'\n"
     ]
    }
   ],
   "source": [
    "concatenated_examples = {\n",
    "    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()\n",
    "}\n",
    "total_length = len(concatenated_examples[\"input_ids\"])\n",
    "print(f\"'>>> Concatenated reviews length: {total_length}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0181e407-c4f8-4d22-83ce-db9c05b1a96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # Create a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfe3f02b-d454-459b-b37d-5f3af6d03b8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013381481170654297,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 4,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "062ed2c61b2748c783f1c017ec7813ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011762619018554688,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d4f21004635461490acd9d5539a9d44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 13865\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 433\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets = tokenized_dataset.map(group_texts, batched=True)\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46773083-a305-4650-a205-f67c2c122f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6492aa3-eb74-4be2-bf87-721a52ce852d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'>>> <s> En Mayo de 1997, una mujer de 29 años<mask> edad fue<mask>, en<mask> centro<mask> de 1935 carcinoma de la<mask> suprarrenal izquierda clínicamente<mask> funcionante que se manifestó clínicamente como molestias en el flanco izquierdo,<mask> específicas, en el postparto inmediato<mask> la ecografía y la tomografía axial computerizada<mask> mostraron una masa suprarrenal izquierda<mask> 10 cm percibida., sólida y con áreas de calcificación<mask><mask><mask> su<mask>,<mask> la<mask> de tórax y la gammagrafía ósea normales. En los análisis, presentaba ligero aumento de la cortisoluria (284.5 mcgr<mask>24h<mask> y de 17-OH-esteroides en orina (12.'\n",
      "\n",
      "'>>> 7 mcg./24h.), sin síntomas de hipercortisolismo sistémico<mask> Se realizó resección completa<mask> la tumoración, con el diagnóstico histológico<mask> carcinoma suprarrenal de 10 x<mask> x 5 cmts. (215 grs.) bien encapsulado, aunque con<mask> vascular, amplias zonas de necrosis y un índice mitótico de 5.8/50<mask> el estudio histoquímico fue (+) para<mask>imentina y (-)gow Citoqueratina, EMA, Cromogranina y Sm<mask> 100. Se etiquetó<mask> Estadio II y Grado II. Treinta<mask> después, la paciente<mask> en los estudios radiológicos de seguimiento una masa en el lecho de resección'\n"
     ]
    }
   ],
   "source": [
    "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "for sample in samples:\n",
    "    _ = sample.pop(\"word_ids\")\n",
    "\n",
    "for chunk in data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b702405b-6458-4c6e-a6a0-19d350926b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "batch_size = 8\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(lm_datasets[\"train\"]) // batch_size\n",
    "model_name = modelCheckpoint.split(\"/\")[-1]\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{model_name}-finetuned-clinais-v2\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    push_to_hub=True,\n",
    "    fp16=True,\n",
    "    logging_steps=logging_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "646c620b-7d69-40b1-8d25-00aad25efd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(modelCheckpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf9fdec2-5360-4ee5-8f9e-6652c821e855",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joheras/.local/lib/python3.10/site-packages/huggingface_hub/repository.py:705: FutureWarning: Creating a repository through 'clone_from' is deprecated and will be removed in v0.11.\n",
      "  warnings.warn(\n",
      "Cloning https://huggingface.co/joheras/bsc-bio-ehr-es-finetuned-clinais-v2 into local empty directory.\n",
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"val\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47a5ba24-cbcd-453b-bea0-2c0ca3d91862",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 433\n",
      "  Batch size = 16\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='56' max='28' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [28/28 01:50]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjoheras\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/joheras/CLINAIS/wandb/run-20230503_083142-3nsr7zdr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/joheras/huggingface/runs/3nsr7zdr\" target=\"_blank\">bsc-bio-ehr-es-finetuned-clinais-v2</a></strong> to <a href=\"https://wandb.ai/joheras/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 7.10\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51e3f623-605d-41d1-806d-8ca3c3565362",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "/home/joheras/.local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 13865\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4335\n",
      "  Number of trainable parameters = 124695126\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4335' max='4335' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4335/4335 08:04, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.356743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.415500</td>\n",
       "      <td>1.321178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.415500</td>\n",
       "      <td>1.297978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.326300</td>\n",
       "      <td>1.262797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.326300</td>\n",
       "      <td>1.269203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-v2/checkpoint-500\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-v2/checkpoint-500/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-v2/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-v2/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-v2/checkpoint-500/special_tokens_map.json\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-v2/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-v2/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 433\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-v2/checkpoint-1000\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-v2/checkpoint-1000/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-v2/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-v2/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-v2/checkpoint-1000/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-v2/checkpoint-1500\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-v2/checkpoint-1500/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-v2/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-v2/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-v2/checkpoint-1500/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 433\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-v2/checkpoint-2000\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-v2/checkpoint-2000/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-v2/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-v2/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-v2/checkpoint-2000/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-v2/checkpoint-2500\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-v2/checkpoint-2500/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-v2/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-v2/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-v2/checkpoint-2500/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 433\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-v2/checkpoint-3000\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-v2/checkpoint-3000/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-v2/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-v2/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-v2/checkpoint-3000/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 433\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-v2/checkpoint-3500\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-v2/checkpoint-3500/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-v2/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-v2/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-v2/checkpoint-3500/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-v2/checkpoint-4000\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-v2/checkpoint-4000/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-v2/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-v2/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-v2/checkpoint-4000/special_tokens_map.json\n",
      "/grupoa/config/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 433\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4335, training_loss=1.355947626063293, metrics={'train_runtime': 484.7874, 'train_samples_per_second': 143.001, 'train_steps_per_second': 8.942, 'total_flos': 4562719401024000.0, 'train_loss': 1.355947626063293, 'epoch': 5.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a459535-f049-4e80-b6df-3eb8ea53e55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: word_ids. If word_ids are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 433\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28' max='28' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [28/28 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 3.83\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dab37a4b-6ba8-40c2-b570-a57d653ba06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to bsc-bio-ehr-es-finetuned-clinais-v2\n",
      "Configuration saved in bsc-bio-ehr-es-finetuned-clinais-v2/config.json\n",
      "Model weights saved in bsc-bio-ehr-es-finetuned-clinais-v2/pytorch_model.bin\n",
      "tokenizer config file saved in bsc-bio-ehr-es-finetuned-clinais-v2/tokenizer_config.json\n",
      "Special tokens file saved in bsc-bio-ehr-es-finetuned-clinais-v2/special_tokens_map.json\n",
      "Several commits (2) will be pushed upstream.\n",
      "The progress bars may be unreliable.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.018495559692382812,
       "initial": 1,
       "n": 1,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Upload file pytorch_model.bin",
       "rate": null,
       "total": 498854201,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebc12f640e974a73a7544ae2d92c9c76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 1.00/476M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011535882949829102,
       "initial": 1,
       "n": 1,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Upload file runs/May03_08-31-20_minion/events.out.tfevents.1683095500.minion.533668.0",
       "rate": null,
       "total": 6069,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "997ea94eb3c6498fb2245fbe07936815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/May03_08-31-20_minion/events.out.tfevents.1683095500.minion.533668.0:   0%|          | 1.00/5…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011388778686523438,
       "initial": 1,
       "n": 1,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Upload file runs/May03_08-31-20_minion/events.out.tfevents.1683095994.minion.533668.2",
       "rate": null,
       "total": 311,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc5f21b280f5438aa0d9038a67bd11f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/May03_08-31-20_minion/events.out.tfevents.1683095994.minion.533668.2:   0%|          | 1.00/3…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/joheras/bsc-bio-ehr-es-finetuned-clinais-v2\n",
      "   dbc70fc..24459f2  main -> main\n",
      "\n",
      "Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Masked Language Modeling', 'type': 'fill-mask'}}\n",
      "To https://huggingface.co/joheras/bsc-bio-ehr-es-finetuned-clinais-v2\n",
      "   24459f2..7212c7e  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/joheras/bsc-bio-ehr-es-finetuned-clinais-v2/commit/24459f27f79351431da3fb00e26f5fdb1e0e4a73'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bdf8a5ce-b3a0-4dd0-9ebc-cc8528a5454d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf bsc-bio-ehr-es-finetuned-clinais-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aaf941-8fe5-484e-b953-aeae28ec8a1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai]",
   "language": "python",
   "name": "conda-env-fastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
